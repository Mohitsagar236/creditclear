{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a781f3d",
   "metadata": {},
   "source": [
    "# Credit Risk Model - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs initial exploratory data analysis on the credit risk datasets:\n",
    "- Loading and examining application train data\n",
    "- Loading and examining bureau data\n",
    "- Basic data quality checks\n",
    "- Missing value analysis\n",
    "\n",
    "## Data Loading and Initial Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display all columns and format floats\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set up data paths\n",
    "DATA_DIR = Path('../data/raw')\n",
    "APPLICATION_TRAIN_PATH = DATA_DIR / 'application_train.csv'\n",
    "BUREAU_PATH = DATA_DIR / 'bureau.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16375c",
   "metadata": {},
   "source": [
    "## Application Train Data\n",
    "\n",
    "Let's load and examine the application train dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ca2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load application train data\n",
    "application_train = pd.read_csv(APPLICATION_TRAIN_PATH)\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"First 5 rows of application_train:\")\n",
    "display(application_train.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "display(application_train.info())\n",
    "\n",
    "# Display statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "display(application_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba4f66",
   "metadata": {},
   "source": [
    "## Missing Value Analysis\n",
    "\n",
    "Let's analyze the missing values in the application train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ea34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing_values = application_train.isnull().sum()\n",
    "missing_percentage = (missing_values / len(application_train)) * 100\n",
    "\n",
    "# Create a summary DataFrame of missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Sort by missing percentage in descending order and display only columns with missing values\n",
    "missing_summary = missing_summary[missing_summary['Missing Values'] > 0].sort_values(\n",
    "    'Missing Percentage', \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "print(\"Columns with missing values:\")\n",
    "display(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264c0a9",
   "metadata": {},
   "source": [
    "## Bureau Data\n",
    "\n",
    "Now let's examine the bureau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bureau data\n",
    "bureau = pd.read_csv(BUREAU_PATH)\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"First 5 rows of bureau data:\")\n",
    "display(bureau.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "display(bureau.info())\n",
    "\n",
    "# Display statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "display(bureau.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738762e",
   "metadata": {},
   "source": [
    "# E-commerce Features Prototype\n",
    "\n",
    "This section demonstrates how to engineer e-commerce features that could be integrated into the credit risk model to enhance prediction accuracy using alternative data sources.\n",
    "\n",
    "## RFM Analysis and Feature Engineering\n",
    "\n",
    "We'll use the Online Retail dataset to create features like:\n",
    "- **Recency**: Days since last purchase\n",
    "- **Frequency**: Number of transactions\n",
    "- **Monetary**: Total spending amount\n",
    "- **Average Order Value**: Mean transaction value\n",
    "- **Purchase Frequency**: Transactions per time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2992222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for e-commerce analysis\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Download the Online Retail dataset from UCI ML Repository\n",
    "def download_online_retail_data():\n",
    "    \"\"\"Download and extract the Online Retail dataset\"\"\"\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"\n",
    "    \n",
    "    print(\"Downloading Online Retail dataset...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to data/raw directory\n",
    "        retail_path = \"../data/raw/online_retail.xlsx\"\n",
    "        with open(retail_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"Dataset downloaded successfully to {retail_path}\")\n",
    "        return retail_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading dataset: {e}\")\n",
    "        # Create sample data if download fails\n",
    "        return create_sample_retail_data()\n",
    "\n",
    "def create_sample_retail_data():\n",
    "    \"\"\"Create sample e-commerce data for demonstration\"\"\"\n",
    "    print(\"Creating sample e-commerce data...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_customers = 1000\n",
    "    n_transactions = 5000\n",
    "    \n",
    "    # Generate sample data\n",
    "    customer_ids = np.random.randint(1, n_customers + 1, n_transactions)\n",
    "    invoice_dates = pd.date_range(start='2022-01-01', end='2023-12-31', periods=n_transactions)\n",
    "    quantities = np.random.randint(1, 50, n_transactions)\n",
    "    unit_prices = np.random.uniform(1, 100, n_transactions)\n",
    "    \n",
    "    sample_data = pd.DataFrame({\n",
    "        'CustomerID': customer_ids,\n",
    "        'InvoiceDate': invoice_dates,\n",
    "        'Quantity': quantities,\n",
    "        'UnitPrice': unit_prices,\n",
    "        'TotalAmount': quantities * unit_prices\n",
    "    })\n",
    "    \n",
    "    # Remove some customers to simulate missing CustomerID\n",
    "    sample_data.loc[sample_data.index % 10 == 0, 'CustomerID'] = np.nan\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# Download or create the dataset\n",
    "try:\n",
    "    retail_file = download_online_retail_data()\n",
    "    if retail_file.endswith('.xlsx'):\n",
    "        retail_data = pd.read_excel(retail_file)\n",
    "    else:\n",
    "        retail_data = retail_file  # Sample data\n",
    "except:\n",
    "    retail_data = create_sample_retail_data()\n",
    "\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and cleaning\n",
    "print(\"Dataset shape:\", retail_data.shape)\n",
    "print(\"\\nColumn names:\", retail_data.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(retail_data.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "retail_data.info()\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(retail_data.isnull().sum())\n",
    "\n",
    "# Clean the data\n",
    "if 'InvoiceNo' in retail_data.columns:\n",
    "    # For real Online Retail dataset\n",
    "    retail_clean = retail_data.copy()\n",
    "    \n",
    "    # Calculate TotalAmount if not present\n",
    "    if 'TotalAmount' not in retail_clean.columns:\n",
    "        retail_clean['TotalAmount'] = retail_clean['Quantity'] * retail_clean['UnitPrice']\n",
    "    \n",
    "    # Remove rows with missing CustomerID\n",
    "    retail_clean = retail_clean.dropna(subset=['CustomerID'])\n",
    "    \n",
    "    # Remove cancelled orders (negative quantities)\n",
    "    retail_clean = retail_clean[retail_clean['Quantity'] > 0]\n",
    "    retail_clean = retail_clean[retail_clean['UnitPrice'] > 0]\n",
    "    \n",
    "    # Convert CustomerID to integer\n",
    "    retail_clean['CustomerID'] = retail_clean['CustomerID'].astype(int)\n",
    "    \n",
    "else:\n",
    "    # For sample data\n",
    "    retail_clean = retail_data.dropna(subset=['CustomerID'])\n",
    "    retail_clean['CustomerID'] = retail_clean['CustomerID'].astype(int)\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {retail_clean.shape}\")\n",
    "print(f\"Number of unique customers: {retail_clean['CustomerID'].nunique()}\")\n",
    "print(f\"Date range: {retail_clean['InvoiceDate'].min()} to {retail_clean['InvoiceDate'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ca9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RFM (Recency, Frequency, Monetary) features\n",
    "def calculate_rfm_features(data, customer_col='CustomerID', date_col='InvoiceDate', amount_col='TotalAmount'):\n",
    "    \"\"\"\n",
    "    Calculate RFM features for each customer\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with transaction data\n",
    "        customer_col: Column name for customer identifier\n",
    "        date_col: Column name for transaction date\n",
    "        amount_col: Column name for transaction amount\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with RFM features per customer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the reference date (latest date + 1 day)\n",
    "    reference_date = data[date_col].max() + timedelta(days=1)\n",
    "    \n",
    "    # Group by customer and calculate RFM metrics\n",
    "    rfm = data.groupby(customer_col).agg({\n",
    "        date_col: lambda x: (reference_date - x.max()).days,  # Recency\n",
    "        amount_col: ['count', 'sum', 'mean']  # Frequency, Monetary, AOV\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    rfm.columns = ['Recency', 'Frequency', 'Monetary', 'AvgOrderValue']\n",
    "    \n",
    "    # Reset index to make CustomerID a column\n",
    "    rfm = rfm.reset_index()\n",
    "    \n",
    "    return rfm\n",
    "\n",
    "# Calculate RFM features\n",
    "rfm_features = calculate_rfm_features(retail_clean)\n",
    "\n",
    "print(\"RFM Features calculated successfully!\")\n",
    "print(f\"Shape: {rfm_features.shape}\")\n",
    "print(\"\\nRFM Features Summary:\")\n",
    "display(rfm_features.describe())\n",
    "\n",
    "print(\"\\nFirst 10 customers:\")\n",
    "display(rfm_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer additional e-commerce features\n",
    "def engineer_ecommerce_features(data, rfm_data, customer_col='CustomerID', date_col='InvoiceDate', amount_col='TotalAmount'):\n",
    "    \"\"\"\n",
    "    Engineer additional e-commerce features beyond basic RFM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate additional features per customer\n",
    "    additional_features = data.groupby(customer_col).agg({\n",
    "        date_col: [\n",
    "            lambda x: x.nunique(),  # Unique shopping days\n",
    "            lambda x: (x.max() - x.min()).days if len(x) > 1 else 0,  # Customer lifespan\n",
    "        ],\n",
    "        amount_col: [\n",
    "            'std',  # Spending volatility\n",
    "            lambda x: x.quantile(0.75) - x.quantile(0.25),  # IQR of spending\n",
    "        ]\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    additional_features.columns = ['UniqueDays', 'CustomerLifespan', 'SpendingVolatility', 'SpendingIQR']\n",
    "    additional_features = additional_features.reset_index()\n",
    "    \n",
    "    # Merge with RFM data\n",
    "    enhanced_features = rfm_data.merge(additional_features, on=customer_col, how='left')\n",
    "    \n",
    "    # Calculate derived features\n",
    "    enhanced_features['PurchaseFrequency'] = enhanced_features['Frequency'] / (enhanced_features['CustomerLifespan'] + 1)\n",
    "    enhanced_features['DaysBetweenPurchases'] = enhanced_features['CustomerLifespan'] / (enhanced_features['Frequency'] - 1)\n",
    "    enhanced_features['DaysBetweenPurchases'] = enhanced_features['DaysBetweenPurchases'].fillna(0)\n",
    "    \n",
    "    # Create RFM scores (1-5 scale)\n",
    "    for col in ['Recency', 'Frequency', 'Monetary']:\n",
    "        if col == 'Recency':\n",
    "            # For recency, lower is better\n",
    "            enhanced_features[f'{col}_Score'] = pd.qcut(enhanced_features[col], 5, labels=[5,4,3,2,1])\n",
    "        else:\n",
    "            # For frequency and monetary, higher is better\n",
    "            enhanced_features[f'{col}_Score'] = pd.qcut(enhanced_features[col].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    # Create combined RFM score\n",
    "    enhanced_features['RFM_Score'] = (\n",
    "        enhanced_features['Recency_Score'].astype(int) * 100 +\n",
    "        enhanced_features['Frequency_Score'].astype(int) * 10 +\n",
    "        enhanced_features['Monetary_Score'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Customer segmentation based on RFM scores\n",
    "    def segment_customers(row):\n",
    "        if row['RFM_Score'] >= 444:\n",
    "            return 'Champions'\n",
    "        elif row['RFM_Score'] >= 334:\n",
    "            return 'Loyal Customers'\n",
    "        elif row['RFM_Score'] >= 224:\n",
    "            return 'Potential Loyalists'\n",
    "        elif row['RFM_Score'] >= 144:\n",
    "            return 'At Risk'\n",
    "        else:\n",
    "            return 'Lost'\n",
    "    \n",
    "    enhanced_features['CustomerSegment'] = enhanced_features.apply(segment_customers, axis=1)\n",
    "    \n",
    "    return enhanced_features\n",
    "\n",
    "# Generate enhanced e-commerce features\n",
    "ecommerce_features = engineer_ecommerce_features(retail_clean, rfm_features)\n",
    "\n",
    "print(\"Enhanced E-commerce Features:\")\n",
    "print(f\"Shape: {ecommerce_features.shape}\")\n",
    "print(f\"Features: {list(ecommerce_features.columns)}\")\n",
    "\n",
    "print(\"\\nCustomer Segmentation:\")\n",
    "print(ecommerce_features['CustomerSegment'].value_counts())\n",
    "\n",
    "print(\"\\nFeature Summary:\")\n",
    "display(ecommerce_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b96140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of e-commerce features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('E-commerce Features Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "# List of numerical features to plot\n",
    "numerical_features = [\n",
    "    'Recency', 'Frequency', 'Monetary', 'AvgOrderValue',\n",
    "    'PurchaseFrequency', 'SpendingVolatility', 'CustomerLifespan',\n",
    "    'UniqueDays', 'DaysBetweenPurchases'\n",
    "]\n",
    "\n",
    "# Create distribution plots\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    # Histogram\n",
    "    axes[row, col].hist(ecommerce_features[feature].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[row, col].set_title(f'{feature} Distribution')\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    \n",
    "    # Add statistics text\n",
    "    mean_val = ecommerce_features[feature].mean()\n",
    "    median_val = ecommerce_features[feature].median()\n",
    "    axes[row, col].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[row, col].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# RFM Score distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Customer segments pie chart\n",
    "plt.subplot(2, 2, 1)\n",
    "segment_counts = ecommerce_features['CustomerSegment'].value_counts()\n",
    "plt.pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Customer Segmentation Distribution')\n",
    "\n",
    "# RFM Score histogram\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(ecommerce_features['RFM_Score'], bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "plt.title('RFM Score Distribution')\n",
    "plt.xlabel('RFM Score')\n",
    "plt.ylabel('Number of Customers')\n",
    "\n",
    "# Recency vs Monetary scatter plot\n",
    "plt.subplot(2, 2, 3)\n",
    "scatter = plt.scatter(ecommerce_features['Recency'], ecommerce_features['Monetary'], \n",
    "                     c=ecommerce_features['Frequency'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Frequency')\n",
    "plt.xlabel('Recency (Days)')\n",
    "plt.ylabel('Monetary Value')\n",
    "plt.title('Recency vs Monetary (colored by Frequency)')\n",
    "\n",
    "# Average Order Value vs Purchase Frequency\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(ecommerce_features['AvgOrderValue'], ecommerce_features['PurchaseFrequency'], alpha=0.6)\n",
    "plt.xlabel('Average Order Value')\n",
    "plt.ylabel('Purchase Frequency')\n",
    "plt.title('AOV vs Purchase Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724dd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis and feature importance\n",
    "# Select numerical features for correlation analysis\n",
    "numerical_cols = [\n",
    "    'Recency', 'Frequency', 'Monetary', 'AvgOrderValue',\n",
    "    'PurchaseFrequency', 'SpendingVolatility', 'CustomerLifespan',\n",
    "    'UniqueDays', 'DaysBetweenPurchases'\n",
    "]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = ecommerce_features[numerical_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('E-commerce Features Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature statistics summary table\n",
    "feature_summary = pd.DataFrame({\n",
    "    'Feature': numerical_cols,\n",
    "    'Mean': [ecommerce_features[col].mean() for col in numerical_cols],\n",
    "    'Std': [ecommerce_features[col].std() for col in numerical_cols],\n",
    "    'Min': [ecommerce_features[col].min() for col in numerical_cols],\n",
    "    'Max': [ecommerce_features[col].max() for col in numerical_cols],\n",
    "    'Skewness': [ecommerce_features[col].skew() for col in numerical_cols]\n",
    "}).round(3)\n",
    "\n",
    "print(\"E-commerce Features Summary Statistics:\")\n",
    "display(feature_summary)\n",
    "\n",
    "# Credit risk implications\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREDIT RISK MODELING IMPLICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "These e-commerce features can enhance credit risk models by providing insights into:\n",
    "\n",
    "1. PAYMENT BEHAVIOR INDICATORS:\n",
    "   • High Recency → Recent shopping activity suggests active financial engagement\n",
    "   • High Frequency → Regular purchasing patterns indicate income stability\n",
    "   • High Monetary → Higher spending capacity suggests better financial position\n",
    "\n",
    "2. FINANCIAL STABILITY METRICS:\n",
    "   • Low SpendingVolatility → Consistent spending patterns indicate financial discipline\n",
    "   • Regular PurchaseFrequency → Predictable behavior reduces default risk\n",
    "   • Long CustomerLifespan → Loyalty and long-term relationship building\n",
    "\n",
    "3. RISK SEGMENTATION:\n",
    "   • Champions → Lowest risk, highest creditworthiness\n",
    "   • Loyal Customers → Low risk, stable payment behavior\n",
    "   • At Risk/Lost → Higher risk, may indicate financial distress\n",
    "\n",
    "4. PREDICTIVE FEATURES FOR INTEGRATION:\n",
    "   • AvgOrderValue can predict disposable income levels\n",
    "   • PurchaseFrequency indicates financial planning capability\n",
    "   • CustomerSegment provides risk-based customer classification\n",
    "\n",
    "NEXT STEPS:\n",
    "- Integrate these features with traditional credit bureau data\n",
    "- Create composite scores combining e-commerce and financial data\n",
    "- Develop models using both traditional and alternative data sources\n",
    "\"\"\")\n",
    "\n",
    "# Save the enhanced features for potential model integration\n",
    "output_path = \"../data/processed/ecommerce_features.csv\"\n",
    "ecommerce_features.to_csv(output_path, index=False)\n",
    "print(f\"\\nE-commerce features saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082c18a",
   "metadata": {},
   "source": [
    "# Telecom Features Prototype\n",
    "\n",
    "This section demonstrates how to engineer telecom features that indicate customer stability and usage consistency for credit risk assessment.\n",
    "\n",
    "## Telecom Data Analysis and Feature Engineering\n",
    "\n",
    "We'll create features from telecom data including:\n",
    "- **Customer Tenure**: Length of relationship with telecom provider\n",
    "- **Usage Consistency**: Stability in calls, SMS, and data usage patterns\n",
    "- **Service Utilization**: Pattern analysis of different telecom services\n",
    "- **Payment Behavior**: Bill payment consistency and patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19946cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create telecom churn dataset\n",
    "def create_telecom_dataset():\n",
    "    \"\"\"\n",
    "    Create a comprehensive telecom dataset for feature engineering.\n",
    "    This simulates a realistic telecom churn dataset with customer usage patterns.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_customers = 2000\n",
    "    \n",
    "    # Generate customer demographics and tenure\n",
    "    customer_ids = range(1, n_customers + 1)\n",
    "    tenure_months = np.random.exponential(24, n_customers).astype(int)  # Average 24 months\n",
    "    tenure_months = np.clip(tenure_months, 1, 120)  # 1 month to 10 years\n",
    "    \n",
    "    # Generate monthly usage data for multiple months (last 12 months)\n",
    "    monthly_data = []\n",
    "    \n",
    "    for customer_id in customer_ids:\n",
    "        customer_tenure = tenure_months[customer_id - 1]\n",
    "        months_to_generate = min(12, customer_tenure)  # Last 12 months or tenure\n",
    "        \n",
    "        # Customer behavior profile (stable vs volatile)\n",
    "        is_stable_customer = np.random.choice([True, False], p=[0.7, 0.3])\n",
    "        \n",
    "        for month in range(months_to_generate):\n",
    "            # Base usage patterns\n",
    "            base_calls = np.random.poisson(100) if is_stable_customer else np.random.poisson(80)\n",
    "            base_sms = np.random.poisson(50) if is_stable_customer else np.random.poisson(40)\n",
    "            base_data_gb = np.random.exponential(5) if is_stable_customer else np.random.exponential(4)\n",
    "            \n",
    "            # Add variability (stable customers have less variance)\n",
    "            variance_factor = 0.2 if is_stable_customer else 0.6\n",
    "            \n",
    "            calls_made = max(0, base_calls + np.random.normal(0, base_calls * variance_factor))\n",
    "            sms_sent = max(0, base_sms + np.random.normal(0, base_sms * variance_factor))\n",
    "            data_used_gb = max(0, base_data_gb + np.random.normal(0, base_data_gb * variance_factor))\n",
    "            \n",
    "            # Calculate bill amount based on usage\n",
    "            call_charges = calls_made * 0.02  # $0.02 per call\n",
    "            sms_charges = sms_sent * 0.01     # $0.01 per SMS\n",
    "            data_charges = data_used_gb * 2   # $2 per GB\n",
    "            base_plan = 25                    # $25 base plan\n",
    "            \n",
    "            monthly_bill = base_plan + call_charges + sms_charges + data_charges\n",
    "            \n",
    "            # Payment behavior (stable customers pay on time more often)\n",
    "            payment_delay_days = 0\n",
    "            if not is_stable_customer and np.random.random() < 0.3:\n",
    "                payment_delay_days = np.random.poisson(5)\n",
    "            \n",
    "            monthly_data.append({\n",
    "                'customer_id': customer_id,\n",
    "                'month': month + 1,\n",
    "                'calls_made': int(calls_made),\n",
    "                'sms_sent': int(sms_sent),\n",
    "                'data_used_gb': round(data_used_gb, 2),\n",
    "                'monthly_bill': round(monthly_bill, 2),\n",
    "                'payment_delay_days': payment_delay_days,\n",
    "                'is_stable_profile': is_stable_customer\n",
    "            })\n",
    "    \n",
    "    monthly_df = pd.DataFrame(monthly_data)\n",
    "    \n",
    "    # Create customer summary dataset\n",
    "    customer_data = []\n",
    "    for customer_id in customer_ids:\n",
    "        tenure = tenure_months[customer_id - 1]\n",
    "        \n",
    "        # Customer monthly income (affects ability to pay)\n",
    "        monthly_income = np.random.normal(3000, 1000)\n",
    "        monthly_income = max(1000, monthly_income)  # Minimum $1000\n",
    "        \n",
    "        # Service plan type\n",
    "        plan_types = ['Basic', 'Standard', 'Premium']\n",
    "        plan_weights = [0.4, 0.4, 0.2]\n",
    "        plan_type = np.random.choice(plan_types, p=plan_weights)\n",
    "        \n",
    "        # Internet service type\n",
    "        internet_types = ['DSL', 'Fiber', 'Cable', 'None']\n",
    "        internet_weights = [0.3, 0.3, 0.3, 0.1]\n",
    "        internet_service = np.random.choice(internet_types, p=internet_weights)\n",
    "        \n",
    "        # Additional services\n",
    "        has_streaming = np.random.choice([True, False], p=[0.6, 0.4])\n",
    "        has_cloud_backup = np.random.choice([True, False], p=[0.3, 0.7])\n",
    "        has_tech_support = np.random.choice([True, False], p=[0.2, 0.8])\n",
    "        \n",
    "        # Contract type\n",
    "        contract_types = ['Month-to-month', 'One year', 'Two year']\n",
    "        contract_weights = [0.5, 0.3, 0.2]\n",
    "        contract_type = np.random.choice(contract_types, p=contract_weights)\n",
    "        \n",
    "        # Customer satisfaction and churn\n",
    "        # Longer tenure and stable usage = less likely to churn\n",
    "        churn_probability = max(0.05, 0.4 - (tenure / 120) * 0.3)\n",
    "        churned = np.random.choice([True, False], p=[churn_probability, 1 - churn_probability])\n",
    "        \n",
    "        customer_data.append({\n",
    "            'customer_id': customer_id,\n",
    "            'tenure_months': tenure,\n",
    "            'monthly_income': round(monthly_income, 2),\n",
    "            'plan_type': plan_type,\n",
    "            'internet_service': internet_service,\n",
    "            'has_streaming': has_streaming,\n",
    "            'has_cloud_backup': has_cloud_backup,\n",
    "            'has_tech_support': has_tech_support,\n",
    "            'contract_type': contract_type,\n",
    "            'churned': churned\n",
    "        })\n",
    "    \n",
    "    customer_df = pd.DataFrame(customer_data)\n",
    "    \n",
    "    return customer_df, monthly_df\n",
    "\n",
    "# Create the telecom datasets\n",
    "print(\"Creating comprehensive telecom dataset...\")\n",
    "telecom_customers, telecom_monthly = create_telecom_dataset()\n",
    "\n",
    "print(f\"Customer data shape: {telecom_customers.shape}\")\n",
    "print(f\"Monthly usage data shape: {telecom_monthly.shape}\")\n",
    "\n",
    "print(\"\\nCustomer Data Sample:\")\n",
    "display(telecom_customers.head())\n",
    "\n",
    "print(\"\\nMonthly Usage Data Sample:\")\n",
    "display(telecom_monthly.head())\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(f\"Total customers: {telecom_customers['customer_id'].nunique()}\")\n",
    "print(f\"Average tenure: {telecom_customers['tenure_months'].mean():.1f} months\")\n",
    "print(f\"Churn rate: {telecom_customers['churned'].mean():.2%}\")\n",
    "print(f\"Total monthly records: {len(telecom_monthly)}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nUsage Statistics:\")\n",
    "print(telecom_monthly[['calls_made', 'sms_sent', 'data_used_gb', 'monthly_bill']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f7cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer stability features from telecom data\n",
    "def create_telecom_stability_features(customer_df, monthly_df):\n",
    "    \"\"\"\n",
    "    Create stability and consistency features from telecom data that indicate creditworthiness.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. TENURE-BASED STABILITY FEATURES\n",
    "    stability_features = customer_df[['customer_id', 'tenure_months']].copy()\n",
    "    \n",
    "    # Convert tenure to years and create stability categories\n",
    "    stability_features['tenure_in_years'] = stability_features['tenure_months'] / 12\n",
    "    \n",
    "    def categorize_tenure_stability(tenure_months):\n",
    "        if tenure_months < 6:\n",
    "            return 'New_Customer'\n",
    "        elif tenure_months < 12:\n",
    "            return 'Short_Term'\n",
    "        elif tenure_months < 24:\n",
    "            return 'Medium_Term'\n",
    "        elif tenure_months < 48:\n",
    "            return 'Long_Term'\n",
    "        else:\n",
    "            return 'Very_Stable'\n",
    "    \n",
    "    stability_features['tenure_category'] = stability_features['tenure_months'].apply(categorize_tenure_stability)\n",
    "    \n",
    "    # 2. USAGE CONSISTENCY FEATURES\n",
    "    # Calculate monthly usage statistics for each customer\n",
    "    usage_consistency = monthly_df.groupby('customer_id').agg({\n",
    "        'calls_made': ['mean', 'std', 'min', 'max', 'count'],\n",
    "        'sms_sent': ['mean', 'std', 'min', 'max'],\n",
    "        'data_used_gb': ['mean', 'std', 'min', 'max'],\n",
    "        'monthly_bill': ['mean', 'std', 'min', 'max']\n",
    "    }).round(2)\n",
    "    \n",
    "    # Flatten column names\n",
    "    usage_consistency.columns = [f\"{col[0]}_{col[1]}\" for col in usage_consistency.columns]\n",
    "    usage_consistency = usage_consistency.reset_index()\n",
    "    \n",
    "    # Calculate coefficient of variation (CV) for usage consistency\n",
    "    # Lower CV = more consistent usage = higher stability\n",
    "    usage_consistency['calls_consistency_cv'] = usage_consistency['calls_made_std'] / usage_consistency['calls_made_mean']\n",
    "    usage_consistency['sms_consistency_cv'] = usage_consistency['sms_sent_std'] / usage_consistency['sms_sent_mean']\n",
    "    usage_consistency['data_consistency_cv'] = usage_consistency['data_used_gb_std'] / usage_consistency['data_used_gb_mean']\n",
    "    usage_consistency['bill_consistency_cv'] = usage_consistency['monthly_bill_std'] / usage_consistency['monthly_bill_mean']\n",
    "    \n",
    "    # Replace inf and NaN values\n",
    "    consistency_cols = ['calls_consistency_cv', 'sms_consistency_cv', 'data_consistency_cv', 'bill_consistency_cv']\n",
    "    for col in consistency_cols:\n",
    "        usage_consistency[col] = usage_consistency[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    # Create overall usage consistency score (lower is better)\n",
    "    usage_consistency['overall_usage_consistency'] = usage_consistency[consistency_cols].mean(axis=1)\n",
    "    \n",
    "    # 3. PAYMENT BEHAVIOR FEATURES\n",
    "    payment_behavior = monthly_df.groupby('customer_id').agg({\n",
    "        'payment_delay_days': ['mean', 'std', 'max', 'sum'],\n",
    "        'month': 'count'  # number of months of data\n",
    "    }).round(2)\n",
    "    \n",
    "    payment_behavior.columns = [f\"payment_{col[0]}_{col[1]}\" if col[0] != 'month' else 'months_of_data' \n",
    "                               for col in payment_behavior.columns]\n",
    "    payment_behavior = payment_behavior.reset_index()\n",
    "    \n",
    "    # Calculate payment reliability score\n",
    "    payment_behavior['payment_reliability_score'] = np.where(\n",
    "        payment_behavior['payment_payment_delay_days_mean'] == 0, 100,\n",
    "        100 - (payment_behavior['payment_payment_delay_days_mean'] * 10)  # Penalty for delays\n",
    "    )\n",
    "    payment_behavior['payment_reliability_score'] = np.clip(payment_behavior['payment_reliability_score'], 0, 100)\n",
    "    \n",
    "    # 4. SERVICE UTILIZATION FEATURES\n",
    "    service_features = customer_df[['customer_id', 'plan_type', 'internet_service', \n",
    "                                   'has_streaming', 'has_cloud_backup', 'has_tech_support', \n",
    "                                   'contract_type']].copy()\n",
    "    \n",
    "    # Count of additional services (indicates engagement)\n",
    "    service_features['additional_services_count'] = (\n",
    "        service_features['has_streaming'].astype(int) +\n",
    "        service_features['has_cloud_backup'].astype(int) +\n",
    "        service_features['has_tech_support'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Contract stability score\n",
    "    contract_stability_map = {\n",
    "        'Month-to-month': 1,\n",
    "        'One year': 2,\n",
    "        'Two year': 3\n",
    "    }\n",
    "    service_features['contract_stability_score'] = service_features['contract_type'].map(contract_stability_map)\n",
    "    \n",
    "    # 5. SPENDING BEHAVIOR FEATURES\n",
    "    spending_features = monthly_df.groupby('customer_id').agg({\n",
    "        'monthly_bill': ['mean', 'std', 'min', 'max', 'sum']\n",
    "    }).round(2)\n",
    "    \n",
    "    spending_features.columns = [f\"spending_{col[1]}\" for col in spending_features.columns]\n",
    "    spending_features = spending_features.reset_index()\n",
    "    \n",
    "    # Calculate spending stability\n",
    "    spending_features['spending_stability'] = 1 / (1 + spending_features['spending_std'])  # Higher = more stable\n",
    "    \n",
    "    # Merge all features\n",
    "    telecom_features = stability_features\n",
    "    feature_dfs = [usage_consistency, payment_behavior, service_features, spending_features]\n",
    "    \n",
    "    for df in feature_dfs:\n",
    "        telecom_features = telecom_features.merge(df, on='customer_id', how='left')\n",
    "    \n",
    "    return telecom_features\n",
    "\n",
    "# Create telecom stability features\n",
    "print(\"Engineering telecom stability and consistency features...\")\n",
    "telecom_features = create_telecom_stability_features(telecom_customers, telecom_monthly)\n",
    "\n",
    "print(f\"Telecom features shape: {telecom_features.shape}\")\n",
    "print(f\"Features created: {telecom_features.columns.tolist()}\")\n",
    "\n",
    "# Display key stability metrics\n",
    "print(\"\\nTenure Stability Distribution:\")\n",
    "print(telecom_features['tenure_category'].value_counts())\n",
    "\n",
    "print(\"\\nKey Stability Features Summary:\")\n",
    "stability_cols = ['tenure_in_years', 'overall_usage_consistency', 'payment_reliability_score', \n",
    "                 'contract_stability_score', 'spending_stability']\n",
    "\n",
    "for col in stability_cols:\n",
    "    if col in telecom_features.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {telecom_features[col].mean():.3f}\")\n",
    "        print(f\"  Std:  {telecom_features[col].std():.3f}\")\n",
    "        print(f\"  Min:  {telecom_features[col].min():.3f}\")\n",
    "        print(f\"  Max:  {telecom_features[col].max():.3f}\")\n",
    "\n",
    "display(telecom_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b23302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced usage consistency and behavioral features\n",
    "def create_advanced_telecom_features(monthly_df):\n",
    "    \"\"\"\n",
    "    Create advanced behavioral features from telecom usage patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. TEMPORAL USAGE PATTERNS\n",
    "    temporal_features = []\n",
    "    \n",
    "    for customer_id in monthly_df['customer_id'].unique():\n",
    "        customer_data = monthly_df[monthly_df['customer_id'] == customer_id].sort_values('month')\n",
    "        \n",
    "        if len(customer_data) < 3:  # Need at least 3 months for trend analysis\n",
    "            continue\n",
    "            \n",
    "        features = {'customer_id': customer_id}\n",
    "        \n",
    "        # Usage trend analysis (increasing, stable, decreasing)\n",
    "        for usage_col in ['calls_made', 'sms_sent', 'data_used_gb']:\n",
    "            usage_values = customer_data[usage_col].values\n",
    "            \n",
    "            # Calculate trend slope\n",
    "            months = np.arange(len(usage_values))\n",
    "            if len(months) > 1:\n",
    "                slope, intercept = np.polyfit(months, usage_values, 1)\n",
    "                features[f'{usage_col}_trend_slope'] = slope\n",
    "                \n",
    "                # Calculate trend consistency (R-squared)\n",
    "                predicted = slope * months + intercept\n",
    "                ss_res = np.sum((usage_values - predicted) ** 2)\n",
    "                ss_tot = np.sum((usage_values - np.mean(usage_values)) ** 2)\n",
    "                r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "                features[f'{usage_col}_trend_consistency'] = r_squared\n",
    "            else:\n",
    "                features[f'{usage_col}_trend_slope'] = 0\n",
    "                features[f'{usage_col}_trend_consistency'] = 0\n",
    "        \n",
    "        # 2. USAGE VOLATILITY FEATURES\n",
    "        for usage_col in ['calls_made', 'sms_sent', 'data_used_gb', 'monthly_bill']:\n",
    "            usage_values = customer_data[usage_col].values\n",
    "            \n",
    "            if len(usage_values) > 1:\n",
    "                # Month-over-month changes\n",
    "                changes = np.diff(usage_values)\n",
    "                features[f'{usage_col}_volatility'] = np.std(changes) if len(changes) > 0 else 0\n",
    "                \n",
    "                # Percentage changes\n",
    "                pct_changes = []\n",
    "                for i in range(1, len(usage_values)):\n",
    "                    if usage_values[i-1] != 0:\n",
    "                        pct_change = (usage_values[i] - usage_values[i-1]) / usage_values[i-1] * 100\n",
    "                        pct_changes.append(abs(pct_change))\n",
    "                \n",
    "                features[f'{usage_col}_pct_volatility'] = np.mean(pct_changes) if pct_changes else 0\n",
    "            else:\n",
    "                features[f'{usage_col}_volatility'] = 0\n",
    "                features[f'{usage_col}_pct_volatility'] = 0\n",
    "        \n",
    "        # 3. PAYMENT BEHAVIOR PATTERNS\n",
    "        payment_delays = customer_data['payment_delay_days'].values\n",
    "        features['payment_delay_frequency'] = (payment_delays > 0).sum() / len(payment_delays)\n",
    "        features['max_consecutive_delays'] = 0\n",
    "        \n",
    "        # Calculate max consecutive payment delays\n",
    "        current_streak = 0\n",
    "        max_streak = 0\n",
    "        for delay in payment_delays:\n",
    "            if delay > 0:\n",
    "                current_streak += 1\n",
    "                max_streak = max(max_streak, current_streak)\n",
    "            else:\n",
    "                current_streak = 0\n",
    "        features['max_consecutive_delays'] = max_streak\n",
    "        \n",
    "        # 4. SERVICE USAGE EFFICIENCY\n",
    "        total_bill = customer_data['monthly_bill'].sum()\n",
    "        total_usage = (\n",
    "            customer_data['calls_made'].sum() * 0.02 +  # Call value\n",
    "            customer_data['sms_sent'].sum() * 0.01 +    # SMS value\n",
    "            customer_data['data_used_gb'].sum() * 2     # Data value\n",
    "        )\n",
    "        \n",
    "        features['usage_efficiency_ratio'] = total_usage / total_bill if total_bill > 0 else 0\n",
    "        \n",
    "        temporal_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(temporal_features)\n",
    "\n",
    "# Create advanced features\n",
    "print(\"Creating advanced telecom behavioral features...\")\n",
    "advanced_features = create_advanced_telecom_features(telecom_monthly)\n",
    "\n",
    "print(f\"Advanced features shape: {advanced_features.shape}\")\n",
    "print(f\"Advanced features: {[col for col in advanced_features.columns if col != 'customer_id']}\")\n",
    "\n",
    "# Merge with main telecom features\n",
    "telecom_features_enhanced = telecom_features.merge(advanced_features, on='customer_id', how='left')\n",
    "\n",
    "print(f\"Enhanced telecom features shape: {telecom_features_enhanced.shape}\")\n",
    "\n",
    "# Fill NaN values for customers with insufficient data\n",
    "feature_cols = [col for col in advanced_features.columns if col != 'customer_id']\n",
    "telecom_features_enhanced[feature_cols] = telecom_features_enhanced[feature_cols].fillna(0)\n",
    "\n",
    "# Display sample of advanced features\n",
    "print(\"\\nAdvanced Features Sample:\")\n",
    "advanced_sample_cols = ['customer_id', 'calls_made_trend_slope', 'data_used_gb_volatility', \n",
    "                       'payment_delay_frequency', 'usage_efficiency_ratio']\n",
    "display(telecom_features_enhanced[advanced_sample_cols].head(10))\n",
    "\n",
    "print(\"\\nAdvanced Features Statistics:\")\n",
    "for col in ['calls_made_trend_slope', 'data_used_gb_volatility', 'payment_delay_frequency', 'usage_efficiency_ratio']:\n",
    "    if col in telecom_features_enhanced.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {telecom_features_enhanced[col].mean():.4f}\")\n",
    "        print(f\"  Std:  {telecom_features_enhanced[col].std():.4f}\")\n",
    "        print(f\"  Min:  {telecom_features_enhanced[col].min():.4f}\")\n",
    "        print(f\"  Max:  {telecom_features_enhanced[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d400068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize telecom features and their relationship to churn/stability\n",
    "# Merge with churn information for analysis\n",
    "telecom_analysis = telecom_features_enhanced.merge(\n",
    "    telecom_customers[['customer_id', 'churned']], \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "fig.suptitle('Telecom Features Analysis for Credit Risk Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Tenure Analysis\n",
    "axes[0, 0].hist(telecom_analysis['tenure_in_years'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Customer Tenure Distribution')\n",
    "axes[0, 0].set_xlabel('Tenure (Years)')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 2. Usage Consistency vs Churn\n",
    "churn_consistency = telecom_analysis.groupby('churned')['overall_usage_consistency'].mean()\n",
    "axes[0, 1].bar(['Retained', 'Churned'], churn_consistency.values, color=['green', 'red'], alpha=0.7)\n",
    "axes[0, 1].set_title('Usage Consistency by Churn Status')\n",
    "axes[0, 1].set_ylabel('Average Usage Consistency')\n",
    "\n",
    "# 3. Payment Reliability vs Churn\n",
    "churn_payment = telecom_analysis.groupby('churned')['payment_reliability_score'].mean()\n",
    "axes[0, 2].bar(['Retained', 'Churned'], churn_payment.values, color=['green', 'red'], alpha=0.7)\n",
    "axes[0, 2].set_title('Payment Reliability by Churn Status')\n",
    "axes[0, 2].set_ylabel('Average Payment Reliability Score')\n",
    "\n",
    "# 4. Tenure Category Distribution\n",
    "tenure_counts = telecom_analysis['tenure_category'].value_counts()\n",
    "axes[1, 0].pie(tenure_counts.values, labels=tenure_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 0].set_title('Customer Tenure Categories')\n",
    "\n",
    "# 5. Contract Stability vs Churn\n",
    "contract_churn = telecom_analysis.groupby('contract_type')['churned'].mean()\n",
    "axes[1, 1].bar(contract_churn.index, contract_churn.values, color='orange', alpha=0.7)\n",
    "axes[1, 1].set_title('Churn Rate by Contract Type')\n",
    "axes[1, 1].set_ylabel('Churn Rate')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Data Usage Volatility Distribution\n",
    "axes[1, 2].hist(telecom_analysis['data_used_gb_volatility'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 2].set_title('Data Usage Volatility Distribution')\n",
    "axes[1, 2].set_xlabel('Data Usage Volatility')\n",
    "axes[1, 2].set_ylabel('Number of Customers')\n",
    "\n",
    "# 7. Monthly Bill Consistency\n",
    "axes[2, 0].scatter(telecom_analysis['spending_mean'], telecom_analysis['spending_stability'], \n",
    "                  c=telecom_analysis['churned'], cmap='RdYlGn_r', alpha=0.6)\n",
    "axes[2, 0].set_title('Bill Amount vs Spending Stability')\n",
    "axes[2, 0].set_xlabel('Average Monthly Bill')\n",
    "axes[2, 0].set_ylabel('Spending Stability')\n",
    "\n",
    "# 8. Additional Services vs Churn\n",
    "service_churn = telecom_analysis.groupby('additional_services_count')['churned'].mean()\n",
    "axes[2, 1].bar(service_churn.index, service_churn.values, color='cyan', alpha=0.7)\n",
    "axes[2, 1].set_title('Churn Rate by Number of Additional Services')\n",
    "axes[2, 1].set_xlabel('Number of Additional Services')\n",
    "axes[2, 1].set_ylabel('Churn Rate')\n",
    "\n",
    "# 9. Payment Delay Frequency Distribution\n",
    "axes[2, 2].hist(telecom_analysis['payment_delay_frequency'], bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[2, 2].set_title('Payment Delay Frequency Distribution')\n",
    "axes[2, 2].set_xlabel('Payment Delay Frequency')\n",
    "axes[2, 2].set_ylabel('Number of Customers')\n",
    "\n",
    "# 10. Usage Trend Analysis\n",
    "axes[3, 0].hist(telecom_analysis['calls_made_trend_slope'], bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
    "axes[3, 0].set_title('Call Usage Trend Distribution')\n",
    "axes[3, 0].set_xlabel('Calls Trend Slope')\n",
    "axes[3, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 11. Correlation heatmap of key features\n",
    "key_telecom_features = [\n",
    "    'tenure_in_years', 'overall_usage_consistency', 'payment_reliability_score',\n",
    "    'contract_stability_score', 'spending_stability', 'data_used_gb_volatility',\n",
    "    'payment_delay_frequency', 'additional_services_count'\n",
    "]\n",
    "\n",
    "correlation_matrix = telecom_analysis[key_telecom_features].corr()\n",
    "im = axes[3, 1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "axes[3, 1].set_xticks(range(len(key_telecom_features)))\n",
    "axes[3, 1].set_yticks(range(len(key_telecom_features)))\n",
    "axes[3, 1].set_xticklabels([f.replace('_', '\\n') for f in key_telecom_features], rotation=45, ha='right')\n",
    "axes[3, 1].set_yticklabels([f.replace('_', '\\n') for f in key_telecom_features])\n",
    "axes[3, 1].set_title('Feature Correlation Matrix')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(key_telecom_features)):\n",
    "    for j in range(len(key_telecom_features)):\n",
    "        axes[3, 1].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', \n",
    "                       ha='center', va='center', fontsize=8)\n",
    "\n",
    "# 12. Risk Score Distribution\n",
    "# Create a simple risk score based on key features\n",
    "telecom_analysis['telecom_risk_score'] = (\n",
    "    (5 - telecom_analysis['tenure_in_years'].clip(0, 5)) * 20 +  # Tenure risk (shorter = higher risk)\n",
    "    telecom_analysis['overall_usage_consistency'] * 30 +         # Usage inconsistency risk\n",
    "    (100 - telecom_analysis['payment_reliability_score']) * 0.3 + # Payment risk\n",
    "    telecom_analysis['payment_delay_frequency'] * 50            # Payment delay risk\n",
    ")\n",
    "\n",
    "axes[3, 2].hist(telecom_analysis['telecom_risk_score'], bins=25, alpha=0.7, color='darkred', edgecolor='black')\n",
    "axes[3, 2].set_title('Telecom Risk Score Distribution')\n",
    "axes[3, 2].set_xlabel('Risk Score (Higher = More Risk)')\n",
    "axes[3, 2].set_ylabel('Number of Customers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TELECOM FEATURES CREDIT RISK INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "KEY STABILITY INDICATORS:\n",
    "\n",
    "1. CUSTOMER TENURE:\n",
    "   • Average tenure: {telecom_analysis['tenure_in_years'].mean():.1f} years\n",
    "   • Customers with >2 years: {(telecom_analysis['tenure_in_years'] > 2).sum()} ({(telecom_analysis['tenure_in_years'] > 2).mean():.1%})\n",
    "   • Churn rate by tenure category:\n",
    "\"\"\")\n",
    "\n",
    "for category in telecom_analysis['tenure_category'].unique():\n",
    "    if pd.notna(category):\n",
    "        churn_rate = telecom_analysis[telecom_analysis['tenure_category'] == category]['churned'].mean()\n",
    "        print(f\"     - {category}: {churn_rate:.1%}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "2. USAGE CONSISTENCY:\n",
    "   • High consistency customers: {(telecom_analysis['overall_usage_consistency'] < 0.3).sum()} ({(telecom_analysis['overall_usage_consistency'] < 0.3).mean():.1%})\n",
    "   • Average usage consistency: {telecom_analysis['overall_usage_consistency'].mean():.3f}\n",
    "\n",
    "3. PAYMENT BEHAVIOR:\n",
    "   • Perfect payment customers: {(telecom_analysis['payment_reliability_score'] == 100).sum()} ({(telecom_analysis['payment_reliability_score'] == 100).mean():.1%})\n",
    "   • Average payment reliability: {telecom_analysis['payment_reliability_score'].mean():.1f}/100\n",
    "\n",
    "4. CONTRACT STABILITY:\n",
    "   • Long-term contracts: {(telecom_analysis['contract_stability_score'] >= 2).sum()} ({(telecom_analysis['contract_stability_score'] >= 2).mean():.1%})\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nCREDIT RISK IMPLICATIONS:\")\n",
    "print(\"\"\"\n",
    "• TENURE → Income Stability: Longer telecom relationships indicate stable residence and income\n",
    "• USAGE CONSISTENCY → Financial Predictability: Consistent usage patterns suggest regular income\n",
    "• PAYMENT RELIABILITY → Credit Discipline: On-time telecom payments predict loan payment behavior\n",
    "• CONTRACT TYPE → Financial Commitment: Long-term contracts show planning capability\n",
    "• SERVICE UTILIZATION → Financial Capacity: Multiple services indicate disposable income\n",
    "\"\"\")\n",
    "\n",
    "# Save telecom features\n",
    "telecom_output_path = \"../data/processed/telecom_features.csv\"\n",
    "telecom_analysis.to_csv(telecom_output_path, index=False)\n",
    "print(f\"\\nTelecom features saved to: {telecom_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate integration of telecom features with credit risk assessment\n",
    "def create_telecom_credit_features(telecom_df):\n",
    "    \"\"\"\n",
    "    Create credit-specific features from telecom data for risk assessment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy for feature engineering\n",
    "    credit_features = telecom_df.copy()\n",
    "    \n",
    "    # 1. STABILITY SCORE (0-100, higher is better)\n",
    "    credit_features['telecom_stability_score'] = (\n",
    "        # Tenure component (40% weight)\n",
    "        (credit_features['tenure_in_years'].clip(0, 5) / 5 * 40) +\n",
    "        \n",
    "        # Usage consistency component (30% weight) - lower consistency CV is better\n",
    "        ((1 - credit_features['overall_usage_consistency'].clip(0, 1)) * 30) +\n",
    "        \n",
    "        # Payment reliability component (30% weight)\n",
    "        (credit_features['payment_reliability_score'] / 100 * 30)\n",
    "    ).round(1)\n",
    "    \n",
    "    # 2. FINANCIAL CAPACITY INDICATORS\n",
    "    # Monthly spending capacity\n",
    "    credit_features['monthly_telecom_spending'] = credit_features['spending_mean']\n",
    "    \n",
    "    # Spending growth trend (positive = increasing capacity)\n",
    "    credit_features['spending_growth_indicator'] = np.where(\n",
    "        credit_features['monthly_bill_trend_slope'] > 0, 1, 0\n",
    "    )\n",
    "    \n",
    "    # Service adoption score (indicates financial capacity)\n",
    "    credit_features['service_adoption_score'] = (\n",
    "        credit_features['additional_services_count'] * 25 +  # Each service worth 25 points\n",
    "        credit_features['contract_stability_score'] * 10     # Contract stability worth 10 points\n",
    "    ).clip(0, 100)\n",
    "    \n",
    "    # 3. BEHAVIORAL RISK INDICATORS\n",
    "    # Payment discipline score\n",
    "    credit_features['payment_discipline_score'] = (\n",
    "        100 - (credit_features['payment_delay_frequency'] * 100)\n",
    "    ).clip(0, 100)\n",
    "    \n",
    "    # Usage volatility risk (higher volatility = higher risk)\n",
    "    max_volatility = credit_features[['calls_made_volatility', 'sms_sent_volatility', 'data_used_gb_volatility']].max(axis=1)\n",
    "    credit_features['usage_volatility_risk'] = (max_volatility / credit_features[['calls_made_volatility', 'sms_sent_volatility', 'data_used_gb_volatility']].max().max() * 100).fillna(0)\n",
    "    \n",
    "    # 4. OVERALL TELECOM CREDIT SCORE\n",
    "    credit_features['telecom_credit_score'] = (\n",
    "        credit_features['telecom_stability_score'] * 0.4 +      # 40% stability\n",
    "        credit_features['payment_discipline_score'] * 0.3 +     # 30% payment behavior\n",
    "        credit_features['service_adoption_score'] * 0.2 +       # 20% financial capacity\n",
    "        (100 - credit_features['usage_volatility_risk']) * 0.1  # 10% consistency\n",
    "    ).round(1)\n",
    "    \n",
    "    # 5. RISK CATEGORIES\n",
    "    def categorize_telecom_risk(score):\n",
    "        if score >= 80:\n",
    "            return 'Low_Risk'\n",
    "        elif score >= 60:\n",
    "            return 'Medium_Risk'\n",
    "        elif score >= 40:\n",
    "            return 'High_Risk'\n",
    "        else:\n",
    "            return 'Very_High_Risk'\n",
    "    \n",
    "    credit_features['telecom_risk_category'] = credit_features['telecom_credit_score'].apply(categorize_telecom_risk)\n",
    "    \n",
    "    return credit_features\n",
    "\n",
    "# Apply credit-specific feature engineering\n",
    "print(\"Creating telecom-based credit risk features...\")\n",
    "telecom_credit_features = create_telecom_credit_features(telecom_analysis)\n",
    "\n",
    "print(f\"Telecom credit features shape: {telecom_credit_features.shape}\")\n",
    "\n",
    "# Analyze credit risk distribution\n",
    "print(\"\\nTelecom Credit Risk Distribution:\")\n",
    "print(telecom_credit_features['telecom_risk_category'].value_counts())\n",
    "\n",
    "print(\"\\nTelecom Credit Score Statistics:\")\n",
    "print(telecom_credit_features['telecom_credit_score'].describe())\n",
    "\n",
    "# Correlation with churn (proxy for credit risk)\n",
    "print(f\"\\nCorrelation between Telecom Credit Score and Churn: {telecom_credit_features['telecom_credit_score'].corr(telecom_credit_features['churned']):.4f}\")\n",
    "\n",
    "# Feature importance for credit assessment\n",
    "credit_risk_features = [\n",
    "    'telecom_stability_score', 'payment_discipline_score', \n",
    "    'service_adoption_score', 'usage_volatility_risk', 'telecom_credit_score'\n",
    "]\n",
    "\n",
    "print(\"\\nCredit Risk Feature Analysis:\")\n",
    "for feature in credit_risk_features:\n",
    "    low_risk_avg = telecom_credit_features[telecom_credit_features['churned'] == False][feature].mean()\n",
    "    high_risk_avg = telecom_credit_features[telecom_credit_features['churned'] == True][feature].mean()\n",
    "    print(f\"{feature}:\")\n",
    "    print(f\"  Low Risk (Retained): {low_risk_avg:.2f}\")\n",
    "    print(f\"  High Risk (Churned): {high_risk_avg:.2f}\")\n",
    "    print(f\"  Difference: {low_risk_avg - high_risk_avg:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualization of credit risk features\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Credit score distribution by risk category\n",
    "plt.subplot(2, 3, 1)\n",
    "for category in telecom_credit_features['telecom_risk_category'].unique():\n",
    "    data = telecom_credit_features[telecom_credit_features['telecom_risk_category'] == category]['telecom_credit_score']\n",
    "    plt.hist(data, alpha=0.6, label=category, bins=15)\n",
    "plt.title('Credit Score Distribution by Risk Category')\n",
    "plt.xlabel('Telecom Credit Score')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.legend()\n",
    "\n",
    "# Stability vs Payment Discipline\n",
    "plt.subplot(2, 3, 2)\n",
    "scatter = plt.scatter(telecom_credit_features['telecom_stability_score'], \n",
    "                     telecom_credit_features['payment_discipline_score'],\n",
    "                     c=telecom_credit_features['churned'], cmap='RdYlGn_r', alpha=0.6)\n",
    "plt.xlabel('Telecom Stability Score')\n",
    "plt.ylabel('Payment Discipline Score')\n",
    "plt.title('Stability vs Payment Discipline')\n",
    "plt.colorbar(scatter, label='Churned')\n",
    "\n",
    "# Risk category vs actual churn\n",
    "plt.subplot(2, 3, 3)\n",
    "churn_by_risk = telecom_credit_features.groupby('telecom_risk_category')['churned'].mean()\n",
    "plt.bar(churn_by_risk.index, churn_by_risk.values, color=['green', 'yellow', 'orange', 'red'])\n",
    "plt.title('Actual Churn Rate by Telecom Risk Category')\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Service adoption vs credit score\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(telecom_credit_features['service_adoption_score'], \n",
    "           telecom_credit_features['telecom_credit_score'], alpha=0.6)\n",
    "plt.xlabel('Service Adoption Score')\n",
    "plt.ylabel('Telecom Credit Score')\n",
    "plt.title('Service Adoption vs Credit Score')\n",
    "\n",
    "# Tenure vs credit score\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(telecom_credit_features['tenure_in_years'], \n",
    "           telecom_credit_features['telecom_credit_score'], alpha=0.6)\n",
    "plt.xlabel('Tenure (Years)')\n",
    "plt.ylabel('Telecom Credit Score')\n",
    "plt.title('Tenure vs Credit Score')\n",
    "\n",
    "# Feature importance comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "feature_importance = []\n",
    "for feature in ['telecom_stability_score', 'payment_discipline_score', 'service_adoption_score']:\n",
    "    corr_with_score = abs(telecom_credit_features[feature].corr(telecom_credit_features['telecom_credit_score']))\n",
    "    feature_importance.append(corr_with_score)\n",
    "\n",
    "plt.bar(['Stability', 'Payment', 'Service'], feature_importance, color=['blue', 'green', 'purple'])\n",
    "plt.title('Feature Importance for Credit Score')\n",
    "plt.ylabel('Correlation with Credit Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TELECOM FEATURES FOR CREDIT RISK ASSESSMENT - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "FEATURE ENGINEERING COMPLETE:\n",
    "\n",
    "1. STABILITY FEATURES:\n",
    "   ✓ Customer Tenure ({telecom_credit_features['tenure_in_years'].mean():.1f} years average)\n",
    "   ✓ Usage Consistency (CV-based measurement)\n",
    "   ✓ Service Commitment (contract types and additional services)\n",
    "\n",
    "2. PAYMENT BEHAVIOR FEATURES:\n",
    "   ✓ Payment Reliability Score ({telecom_credit_features['payment_reliability_score'].mean():.1f}/100 average)\n",
    "   ✓ Payment Discipline Score ({telecom_credit_features['payment_discipline_score'].mean():.1f}/100 average)\n",
    "   ✓ Payment Delay Patterns\n",
    "\n",
    "3. FINANCIAL CAPACITY INDICATORS:\n",
    "   ✓ Monthly Spending Patterns (${telecom_credit_features['monthly_telecom_spending'].mean():.2f} average)\n",
    "   ✓ Service Adoption Score ({telecom_credit_features['service_adoption_score'].mean():.1f}/100 average)\n",
    "   ✓ Spending Growth Trends\n",
    "\n",
    "4. RISK ASSESSMENT:\n",
    "   ✓ Comprehensive Telecom Credit Score ({telecom_credit_features['telecom_credit_score'].mean():.1f}/100 average)\n",
    "   ✓ Risk Categorization (Low/Medium/High/Very High)\n",
    "   ✓ Correlation with churn behavior: {telecom_credit_features['telecom_credit_score'].corr(telecom_credit_features['churned']):.3f}\n",
    "\n",
    "CREDIT RISK INSIGHTS:\n",
    "• Longer tenure customers show {(1-telecom_credit_features[telecom_credit_features['tenure_in_years'] > 2]['churned'].mean()):.1%} retention\n",
    "• Consistent usage patterns reduce risk by {((telecom_credit_features[telecom_credit_features['overall_usage_consistency'] < 0.3]['churned'].mean() - telecom_credit_features['churned'].mean()) * -100):.1f} percentage points\n",
    "• Perfect payment history customers have {(1-telecom_credit_features[telecom_credit_features['payment_reliability_score'] == 100]['churned'].mean()):.1%} retention rate\n",
    "\n",
    "INTEGRATION READY: These features can now be integrated with traditional credit data for enhanced risk assessment.\n",
    "\"\"\")\n",
    "\n",
    "# Save final telecom credit features\n",
    "final_output_path = \"../data/processed/telecom_credit_features.csv\"\n",
    "telecom_credit_features.to_csv(final_output_path, index=False)\n",
    "print(f\"\\nFinal telecom credit features saved to: {final_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa4c34",
   "metadata": {},
   "source": [
    "# 🗺️ Mobility Features - GPS Data Analysis\n",
    "\n",
    "Mobility patterns can provide valuable insights for credit risk assessment:\n",
    "- **Commute stability**: Regular work-home patterns indicate employment stability\n",
    "- **Location consistency**: Frequent locations suggest residential and employment stability\n",
    "- **Travel patterns**: Distance between work/home can indicate income potential and lifestyle\n",
    "\n",
    "We'll prototype features using GPS trajectory data to identify:\n",
    "1. Work location (most frequent daytime location)\n",
    "2. Home location (most frequent nighttime location)  \n",
    "3. Commute distance and patterns\n",
    "4. Location stability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for GPS data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, time\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import geodesic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have access to the actual Geolife dataset, we'll simulate realistic GPS trajectory data\n",
    "# This simulates a user's GPS data over 30 days with realistic movement patterns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define realistic locations (Beijing coordinates similar to Geolife dataset)\n",
    "home_location = [39.9042, 116.4074]  # Base home location\n",
    "work_location = [39.9388, 116.3974]  # Base work location (about 4km from home)\n",
    "\n",
    "# Generate 30 days of GPS trajectories\n",
    "start_date = pd.Timestamp('2024-01-01')\n",
    "gps_data = []\n",
    "\n",
    "for day in range(30):\n",
    "    current_date = start_date + pd.Timedelta(days=day)\n",
    "    \n",
    "    # Generate hourly GPS points for each day\n",
    "    for hour in range(24):\n",
    "        timestamp = current_date + pd.Timedelta(hours=hour, minutes=np.random.randint(0, 60))\n",
    "        \n",
    "        # Determine primary location based on time of day\n",
    "        if 22 <= hour or hour <= 6:  # Nighttime hours (10pm - 6am)\n",
    "            # At home with small random variations\n",
    "            lat = home_location[0] + np.random.normal(0, 0.002)\n",
    "            lon = home_location[1] + np.random.normal(0, 0.002)\n",
    "            location_type = 'home'\n",
    "        elif 9 <= hour <= 17:  # Daytime work hours (9am - 5pm)\n",
    "            # At work with small random variations\n",
    "            lat = work_location[0] + np.random.normal(0, 0.001)\n",
    "            lon = work_location[1] + np.random.normal(0, 0.001)\n",
    "            location_type = 'work'\n",
    "        else:  # Commute or other activities\n",
    "            # Random locations between home and work, or nearby areas\n",
    "            if np.random.random() < 0.7:  # 70% chance of being in transit\n",
    "                # Linear interpolation between home and work with noise\n",
    "                ratio = np.random.random()\n",
    "                lat = home_location[0] * (1-ratio) + work_location[0] * ratio + np.random.normal(0, 0.003)\n",
    "                lon = home_location[1] * (1-ratio) + work_location[1] * ratio + np.random.normal(0, 0.003)\n",
    "                location_type = 'transit'\n",
    "            else:  # Other locations (shopping, dining, etc.)\n",
    "                lat = home_location[0] + np.random.normal(0, 0.01)\n",
    "                lon = home_location[1] + np.random.normal(0, 0.01)\n",
    "                location_type = 'other'\n",
    "        \n",
    "        gps_data.append({\n",
    "            'timestamp': timestamp,\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'hour': hour,\n",
    "            'location_type': location_type  # This is just for our reference, not normally available\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "gps_df = pd.DataFrame(gps_data)\n",
    "gps_df['date'] = gps_df['timestamp'].dt.date\n",
    "gps_df['time'] = gps_df['timestamp'].dt.time\n",
    "\n",
    "print(f\"📊 Generated {len(gps_df)} GPS trajectory points over {gps_df['date'].nunique()} days\")\n",
    "print(f\"📍 Latitude range: {gps_df['latitude'].min():.4f} to {gps_df['latitude'].max():.4f}\")\n",
    "print(f\"📍 Longitude range: {gps_df['longitude'].min():.4f} to {gps_df['longitude'].max():.4f}\")\n",
    "\n",
    "gps_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b59bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Identify Work Location (Most frequent location during daytime hours 9am-5pm)\n",
    "\n",
    "# Filter GPS data for daytime work hours (9am - 5pm)\n",
    "work_hours_data = gps_df[(gps_df['hour'] >= 9) & (gps_df['hour'] <= 17)].copy()\n",
    "\n",
    "print(f\"📊 Analyzing {len(work_hours_data)} GPS points during work hours (9am-5pm)\")\n",
    "\n",
    "# Use DBSCAN clustering to find location clusters during work hours\n",
    "work_coords = work_hours_data[['latitude', 'longitude']].values\n",
    "\n",
    "# DBSCAN parameters: eps in degrees (roughly 100 meters), min_samples for cluster\n",
    "eps_degrees = 0.001  # Approximately 100 meters at Beijing latitude\n",
    "min_samples = 5     # Minimum points to form a cluster\n",
    "\n",
    "dbscan_work = DBSCAN(eps=eps_degrees, min_samples=min_samples)\n",
    "work_clusters = dbscan_work.fit_predict(work_coords)\n",
    "\n",
    "work_hours_data['cluster'] = work_clusters\n",
    "\n",
    "# Find the most frequent cluster (excluding noise points labeled as -1)\n",
    "cluster_counts = pd.Series(work_clusters).value_counts()\n",
    "cluster_counts = cluster_counts[cluster_counts.index != -1]  # Remove noise\n",
    "\n",
    "if len(cluster_counts) > 0:\n",
    "    most_frequent_work_cluster = cluster_counts.index[0]\n",
    "    work_cluster_data = work_hours_data[work_hours_data['cluster'] == most_frequent_work_cluster]\n",
    "    \n",
    "    # Calculate centroid of the most frequent work cluster\n",
    "    work_location_inferred = [\n",
    "        work_cluster_data['latitude'].mean(),\n",
    "        work_cluster_data['longitude'].mean()\n",
    "    ]\n",
    "    \n",
    "    print(f\"🏢 Inferred WORK location: ({work_location_inferred[0]:.6f}, {work_location_inferred[1]:.6f})\")\n",
    "    print(f\"📍 Based on {len(work_cluster_data)} GPS points in largest cluster\")\n",
    "    print(f\"🕒 Time range: {work_cluster_data['timestamp'].min()} to {work_cluster_data['timestamp'].max()}\")\n",
    "    \n",
    "    # Show cluster statistics\n",
    "    print(f\"\\n📊 Work Hours Clustering Results:\")\n",
    "    print(f\"   - Total clusters found: {len(cluster_counts)}\")\n",
    "    print(f\"   - Largest cluster size: {cluster_counts.iloc[0]} points\")\n",
    "    print(f\"   - Noise points: {sum(work_clusters == -1)} points\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No significant location clusters found during work hours\")\n",
    "    work_location_inferred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Identify Home Location (Most frequent location during nighttime hours 10pm-6am)\n",
    "\n",
    "# Filter GPS data for nighttime hours (10pm - 6am)\n",
    "night_hours_data = gps_df[\n",
    "    (gps_df['hour'] >= 22) | (gps_df['hour'] <= 6)\n",
    "].copy()\n",
    "\n",
    "print(f\"📊 Analyzing {len(night_hours_data)} GPS points during nighttime hours (10pm-6am)\")\n",
    "\n",
    "# Use DBSCAN clustering for nighttime locations\n",
    "night_coords = night_hours_data[['latitude', 'longitude']].values\n",
    "\n",
    "dbscan_night = DBSCAN(eps=eps_degrees, min_samples=min_samples)\n",
    "night_clusters = dbscan_night.fit_predict(night_coords)\n",
    "\n",
    "night_hours_data['cluster'] = night_clusters\n",
    "\n",
    "# Find the most frequent cluster during nighttime\n",
    "night_cluster_counts = pd.Series(night_clusters).value_counts()\n",
    "night_cluster_counts = night_cluster_counts[night_cluster_counts.index != -1]  # Remove noise\n",
    "\n",
    "if len(night_cluster_counts) > 0:\n",
    "    most_frequent_night_cluster = night_cluster_counts.index[0]\n",
    "    home_cluster_data = night_hours_data[night_hours_data['cluster'] == most_frequent_night_cluster]\n",
    "    \n",
    "    # Calculate centroid of the most frequent nighttime cluster\n",
    "    home_location_inferred = [\n",
    "        home_cluster_data['latitude'].mean(),\n",
    "        home_cluster_data['longitude'].mean()\n",
    "    ]\n",
    "    \n",
    "    print(f\"🏠 Inferred HOME location: ({home_location_inferred[0]:.6f}, {home_location_inferred[1]:.6f})\")\n",
    "    print(f\"📍 Based on {len(home_cluster_data)} GPS points in largest cluster\")\n",
    "    print(f\"🕒 Time range: {home_cluster_data['timestamp'].min()} to {home_cluster_data['timestamp'].max()}\")\n",
    "    \n",
    "    # Show cluster statistics\n",
    "    print(f\"\\n📊 Night Hours Clustering Results:\")\n",
    "    print(f\"   - Total clusters found: {len(night_cluster_counts)}\")\n",
    "    print(f\"   - Largest cluster size: {night_cluster_counts.iloc[0]} points\")\n",
    "    print(f\"   - Noise points: {sum(night_clusters == -1)} points\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No significant location clusters found during nighttime hours\")\n",
    "    home_location_inferred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate distance between inferred work and home locations\n",
    "\n",
    "if work_location_inferred and home_location_inferred:\n",
    "    # Calculate geodesic distance between work and home\n",
    "    home_work_distance = geodesic(home_location_inferred, work_location_inferred).kilometers\n",
    "    \n",
    "    print(f\"🗺️ MOBILITY ANALYSIS RESULTS\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"🏠 Home Location: ({home_location_inferred[0]:.6f}, {home_location_inferred[1]:.6f})\")\n",
    "    print(f\"🏢 Work Location: ({work_location_inferred[0]:.6f}, {work_location_inferred[1]:.6f})\")\n",
    "    print(f\"📏 Home-Work Distance: {home_work_distance:.2f} km\")\n",
    "    \n",
    "    # Classify commute distance for credit risk assessment\n",
    "    if home_work_distance < 5:\n",
    "        commute_risk = \"Low\"\n",
    "        commute_desc = \"Short commute - likely stable local employment\"\n",
    "    elif home_work_distance < 15:\n",
    "        commute_risk = \"Medium\"\n",
    "        commute_desc = \"Moderate commute - normal urban pattern\"\n",
    "    elif home_work_distance < 30:\n",
    "        commute_risk = \"Medium-High\"\n",
    "        commute_desc = \"Long commute - may indicate job instability or high transport costs\"\n",
    "    else:\n",
    "        commute_risk = \"High\"\n",
    "        commute_desc = \"Very long commute - potential employment or financial stress\"\n",
    "    \n",
    "    print(f\"🚗 Commute Risk Level: {commute_risk}\")\n",
    "    print(f\"💡 Interpretation: {commute_desc}\")\n",
    "    \n",
    "    # Additional mobility metrics for credit assessment\n",
    "    work_day_consistency = len(work_cluster_data) / len(work_hours_data) * 100\n",
    "    home_night_consistency = len(home_cluster_data) / len(night_hours_data) * 100\n",
    "    \n",
    "    print(f\"\\n📊 STABILITY METRICS:\")\n",
    "    print(f\"   - Work Location Consistency: {work_day_consistency:.1f}% of work hours\")\n",
    "    print(f\"   - Home Location Consistency: {home_night_consistency:.1f}% of night hours\")\n",
    "    \n",
    "    if work_day_consistency > 70 and home_night_consistency > 80:\n",
    "        stability_score = \"High\"\n",
    "        stability_desc = \"Regular patterns suggest employment and residential stability\"\n",
    "    elif work_day_consistency > 50 and home_night_consistency > 60:\n",
    "        stability_score = \"Medium\"\n",
    "        stability_desc = \"Moderate regularity in daily patterns\"\n",
    "    else:\n",
    "        stability_score = \"Low\"\n",
    "        stability_desc = \"Irregular patterns may indicate instability\"\n",
    "    \n",
    "    print(f\"📈 Stability Score: {stability_score}\")\n",
    "    print(f\"💡 Assessment: {stability_desc}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Could not calculate distance - insufficient location data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize GPS trajectory patterns and identified locations\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('🗺️ GPS Mobility Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: All GPS points colored by time of day\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(gps_df['longitude'], gps_df['latitude'], \n",
    "                     c=gps_df['hour'], cmap='viridis', alpha=0.6, s=20)\n",
    "plt.colorbar(scatter, ax=ax1, label='Hour of Day')\n",
    "ax1.set_title('📍 All GPS Trajectories by Time of Day')\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "\n",
    "# Plot 2: Work hours clustering\n",
    "ax2 = axes[0, 1]\n",
    "if work_location_inferred:\n",
    "    # Color by cluster\n",
    "    colors = ['red' if c == -1 else plt.cm.Set1(c) for c in work_hours_data['cluster']]\n",
    "    ax2.scatter(work_hours_data['longitude'], work_hours_data['latitude'], \n",
    "               c=colors, alpha=0.7, s=30)\n",
    "    ax2.scatter(work_location_inferred[1], work_location_inferred[0], \n",
    "               color='red', s=200, marker='*', label='Inferred Work Location')\n",
    "    ax2.legend()\n",
    "ax2.set_title('🏢 Work Hours Clustering (9am-5pm)')\n",
    "ax2.set_xlabel('Longitude')\n",
    "ax2.set_ylabel('Latitude')\n",
    "\n",
    "# Plot 3: Night hours clustering\n",
    "ax3 = axes[1, 0]\n",
    "if home_location_inferred:\n",
    "    colors = ['red' if c == -1 else plt.cm.Set1(c) for c in night_hours_data['cluster']]\n",
    "    ax3.scatter(night_hours_data['longitude'], night_hours_data['latitude'], \n",
    "               c=colors, alpha=0.7, s=30)\n",
    "    ax3.scatter(home_location_inferred[1], home_location_inferred[0], \n",
    "               color='blue', s=200, marker='*', label='Inferred Home Location')\n",
    "    ax3.legend()\n",
    "ax3.set_title('🏠 Night Hours Clustering (10pm-6am)')\n",
    "ax3.set_xlabel('Longitude')\n",
    "ax3.set_ylabel('Latitude')\n",
    "\n",
    "# Plot 4: Home-Work relationship\n",
    "ax4 = axes[1, 1]\n",
    "if work_location_inferred and home_location_inferred:\n",
    "    # Plot all points in light gray\n",
    "    ax4.scatter(gps_df['longitude'], gps_df['latitude'], \n",
    "               color='lightgray', alpha=0.3, s=10, label='All GPS Points')\n",
    "    \n",
    "    # Highlight home and work locations\n",
    "    ax4.scatter(home_location_inferred[1], home_location_inferred[0], \n",
    "               color='blue', s=200, marker='H', label=f'Home Location')\n",
    "    ax4.scatter(work_location_inferred[1], work_location_inferred[0], \n",
    "               color='red', s=200, marker='W', label=f'Work Location')\n",
    "    \n",
    "    # Draw line between home and work\n",
    "    ax4.plot([home_location_inferred[1], work_location_inferred[1]], \n",
    "            [home_location_inferred[0], work_location_inferred[0]], \n",
    "            'g--', linewidth=2, alpha=0.7, \n",
    "            label=f'Distance: {home_work_distance:.2f} km')\n",
    "    \n",
    "    ax4.legend()\n",
    "ax4.set_title('🚗 Home-Work Commute Pattern')\n",
    "ax4.set_xlabel('Longitude')\n",
    "ax4.set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a summary mobility features dataframe\n",
    "if work_location_inferred and home_location_inferred:\n",
    "    mobility_features = pd.DataFrame({\n",
    "        'user_id': ['user_001'],\n",
    "        'home_lat': [home_location_inferred[0]],\n",
    "        'home_lon': [home_location_inferred[1]],\n",
    "        'work_lat': [work_location_inferred[0]],\n",
    "        'work_lon': [work_location_inferred[1]],\n",
    "        'commute_distance_km': [home_work_distance],\n",
    "        'work_consistency_pct': [work_day_consistency],\n",
    "        'home_consistency_pct': [home_night_consistency],\n",
    "        'commute_risk_level': [commute_risk],\n",
    "        'stability_score': [stability_score]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n📊 MOBILITY FEATURES SUMMARY:\")\n",
    "    print(mobility_features.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Advanced Mobility Features for Credit Risk Assessment\n",
    "\n",
    "def extract_advanced_mobility_features(gps_data):\n",
    "    \"\"\"\n",
    "    Extract comprehensive mobility features relevant for credit risk assessment\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Basic location statistics\n",
    "    features['total_locations'] = len(gps_data)\n",
    "    features['unique_days'] = gps_data['date'].nunique()\n",
    "    features['avg_points_per_day'] = features['total_locations'] / features['unique_days']\n",
    "    \n",
    "    # Geographic spread\n",
    "    lat_range = gps_data['latitude'].max() - gps_data['latitude'].min()\n",
    "    lon_range = gps_data['longitude'].max() - gps_data['longitude'].min()\n",
    "    features['location_spread_lat'] = lat_range\n",
    "    features['location_spread_lon'] = lon_range\n",
    "    features['location_spread_total'] = lat_range + lon_range\n",
    "    \n",
    "    # Time-based patterns\n",
    "    hourly_activity = gps_data.groupby('hour').size()\n",
    "    features['peak_activity_hour'] = hourly_activity.idxmax()\n",
    "    features['activity_variance'] = hourly_activity.var()\n",
    "    \n",
    "    # Weekend vs weekday patterns\n",
    "    gps_data['weekday'] = pd.to_datetime(gps_data['timestamp']).dt.weekday\n",
    "    weekday_data = gps_data[gps_data['weekday'] < 5]  # Mon-Fri\n",
    "    weekend_data = gps_data[gps_data['weekday'] >= 5]  # Sat-Sun\n",
    "    \n",
    "    if len(weekday_data) > 0 and len(weekend_data) > 0:\n",
    "        features['weekday_mobility'] = len(weekday_data) / 5  # avg per weekday\n",
    "        features['weekend_mobility'] = len(weekend_data) / 2  # avg per weekend day\n",
    "        features['weekend_weekday_ratio'] = features['weekend_mobility'] / features['weekday_mobility']\n",
    "    \n",
    "    # Regular pattern detection\n",
    "    daily_patterns = gps_data.groupby(['date', 'hour']).size().reset_index(name='count')\n",
    "    pattern_consistency = daily_patterns.groupby('hour')['count'].std().mean()\n",
    "    features['pattern_consistency'] = pattern_consistency\n",
    "    \n",
    "    # Mobility radius (distance from centroid)\n",
    "    centroid_lat = gps_data['latitude'].mean()\n",
    "    centroid_lon = gps_data['longitude'].mean()\n",
    "    distances = gps_data.apply(\n",
    "        lambda row: geodesic((centroid_lat, centroid_lon), \n",
    "                           (row['latitude'], row['longitude'])).kilometers, \n",
    "        axis=1\n",
    "    )\n",
    "    features['avg_distance_from_center'] = distances.mean()\n",
    "    features['max_distance_from_center'] = distances.max()\n",
    "    features['mobility_radius_95th'] = distances.quantile(0.95)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract advanced features\n",
    "advanced_features = extract_advanced_mobility_features(gps_df)\n",
    "\n",
    "print(\"🔍 ADVANCED MOBILITY FEATURES:\")\n",
    "print(\"=\" * 50)\n",
    "for feature, value in advanced_features.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{feature:.<30} {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{feature:.<30} {value}\")\n",
    "\n",
    "# Create risk assessment based on mobility patterns\n",
    "def assess_mobility_risk(features, commute_distance=None):\n",
    "    \"\"\"\n",
    "    Assess credit risk based on mobility patterns\n",
    "    \"\"\"\n",
    "    risk_factors = []\n",
    "    risk_score = 0\n",
    "    \n",
    "    # Factor 1: Commute distance\n",
    "    if commute_distance:\n",
    "        if commute_distance > 30:\n",
    "            risk_factors.append(\"Very long commute (>30km)\")\n",
    "            risk_score += 2\n",
    "        elif commute_distance > 15:\n",
    "            risk_factors.append(\"Long commute (15-30km)\")\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Factor 2: Location spread (higher spread = more unstable)\n",
    "    if features['location_spread_total'] > 0.05:\n",
    "        risk_factors.append(\"High geographic dispersion\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Factor 3: Activity consistency\n",
    "    if features['pattern_consistency'] > 2:\n",
    "        risk_factors.append(\"Irregular daily patterns\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Factor 4: Mobility radius\n",
    "    if features['mobility_radius_95th'] > 20:\n",
    "        risk_factors.append(\"Very wide mobility range\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Factor 5: Weekend/weekday ratio\n",
    "    if 'weekend_weekday_ratio' in features:\n",
    "        if features['weekend_weekday_ratio'] > 2:\n",
    "            risk_factors.append(\"Unusual weekend activity pattern\")\n",
    "            risk_score += 1\n",
    "    \n",
    "    # Determine overall risk level\n",
    "    if risk_score <= 1:\n",
    "        risk_level = \"Low\"\n",
    "        risk_description = \"Stable mobility patterns indicate residential and employment stability\"\n",
    "    elif risk_score <= 3:\n",
    "        risk_level = \"Medium\"\n",
    "        risk_description = \"Some irregular patterns but generally stable\"\n",
    "    else:\n",
    "        risk_level = \"High\"\n",
    "        risk_description = \"Multiple indicators of instability in mobility patterns\"\n",
    "    \n",
    "    return {\n",
    "        'risk_score': risk_score,\n",
    "        'risk_level': risk_level,\n",
    "        'risk_factors': risk_factors,\n",
    "        'description': risk_description\n",
    "    }\n",
    "\n",
    "# Perform risk assessment\n",
    "mobility_risk = assess_mobility_risk(advanced_features, \n",
    "                                   home_work_distance if 'home_work_distance' in locals() else None)\n",
    "\n",
    "print(f\"\\n🎯 MOBILITY RISK ASSESSMENT:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Risk Level: {mobility_risk['risk_level']}\")\n",
    "print(f\"Risk Score: {mobility_risk['risk_score']}/6\")\n",
    "print(f\"Description: {mobility_risk['description']}\")\n",
    "\n",
    "if mobility_risk['risk_factors']:\n",
    "    print(f\"\\nRisk Factors Identified:\")\n",
    "    for factor in mobility_risk['risk_factors']:\n",
    "        print(f\"  ⚠️ {factor}\")\n",
    "else:\n",
    "    print(f\"\\n✅ No significant risk factors identified\")\n",
    "\n",
    "print(f\"\\n💡 Credit Risk Implications:\")\n",
    "print(f\"   - Regular home/work patterns suggest employment stability\")\n",
    "print(f\"   - Reasonable commute distance indicates sustainable lifestyle\")\n",
    "print(f\"   - Consistent daily patterns show predictable behavior\")\n",
    "print(f\"   - Limited mobility radius suggests local community ties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea8686",
   "metadata": {},
   "source": [
    "# 📱 Device Features - Mobile Usage Analysis\n",
    "\n",
    "Device usage patterns can provide valuable insights for credit risk assessment:\n",
    "- **Device Quality**: Higher-end devices may indicate better financial status\n",
    "- **Usage Patterns**: App usage and screen time reveal lifestyle and spending habits\n",
    "- **Data Consumption**: Heavy data usage might indicate higher income or tech-savvy users\n",
    "- **Battery Behavior**: Usage intensity patterns show daily routine consistency\n",
    "\n",
    "We'll prototype features using mobile device usage data to extract:\n",
    "1. Operating System and Device Model encoding\n",
    "2. Data usage efficiency metrics \n",
    "3. Battery consumption patterns\n",
    "4. App usage intensity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have access to the actual Mobile Device Usage dataset, we'll simulate realistic device data\n",
    "# This mimics the structure of mobile device usage datasets commonly found on Kaggle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate realistic mobile device usage data\n",
    "n_users = 1000\n",
    "\n",
    "# Device specifications\n",
    "operating_systems = ['iOS', 'Android', 'Other']\n",
    "os_weights = [0.4, 0.55, 0.05]\n",
    "\n",
    "device_models = [\n",
    "    'iPhone 14', 'iPhone 13', 'iPhone 12', 'iPhone SE',\n",
    "    'Samsung Galaxy S23', 'Samsung Galaxy A54', 'Samsung Galaxy A34',\n",
    "    'Google Pixel 7', 'Google Pixel 6a', 'OnePlus 11',\n",
    "    'Xiaomi 13', 'Huawei P50', 'Other Android', 'Other Device'\n",
    "]\n",
    "\n",
    "# Generate device data\n",
    "device_data = []\n",
    "\n",
    "for i in range(n_users):\n",
    "    # Select OS first, then compatible device\n",
    "    os = np.random.choice(operating_systems, p=os_weights)\n",
    "    \n",
    "    if os == 'iOS':\n",
    "        device = np.random.choice(['iPhone 14', 'iPhone 13', 'iPhone 12', 'iPhone SE'], \n",
    "                                p=[0.3, 0.35, 0.25, 0.1])\n",
    "    elif os == 'Android':\n",
    "        device = np.random.choice([\n",
    "            'Samsung Galaxy S23', 'Samsung Galaxy A54', 'Samsung Galaxy A34',\n",
    "            'Google Pixel 7', 'Google Pixel 6a', 'OnePlus 11',\n",
    "            'Xiaomi 13', 'Huawei P50', 'Other Android'\n",
    "        ], p=[0.2, 0.15, 0.15, 0.1, 0.1, 0.08, 0.08, 0.05, 0.09])\n",
    "    else:\n",
    "        device = 'Other Device'\n",
    "    \n",
    "    # Generate usage patterns with some correlation to device type\n",
    "    device_tier = 'premium' if device in ['iPhone 14', 'iPhone 13', 'Samsung Galaxy S23', 'Google Pixel 7', 'OnePlus 11'] else 'standard'\n",
    "    \n",
    "    # App usage patterns\n",
    "    if device_tier == 'premium':\n",
    "        screen_time = np.random.normal(6.5, 2.0)  # Premium users tend to use phones more\n",
    "        num_apps = np.random.poisson(45) + 20\n",
    "        data_usage_mb = np.random.normal(8500, 2500)\n",
    "    else:\n",
    "        screen_time = np.random.normal(4.8, 1.8)\n",
    "        num_apps = np.random.poisson(30) + 15\n",
    "        data_usage_mb = np.random.normal(5500, 2000)\n",
    "    \n",
    "    # Ensure positive values\n",
    "    screen_time = max(1.0, screen_time)\n",
    "    num_apps = max(10, num_apps)\n",
    "    data_usage_mb = max(500, data_usage_mb)\n",
    "    \n",
    "    # Battery and usage patterns\n",
    "    battery_drain = np.random.normal(25, 8)  # % per day\n",
    "    battery_drain = np.clip(battery_drain, 10, 50)\n",
    "    \n",
    "    app_usage_time = screen_time * np.random.uniform(0.7, 0.9)  # Apps don't account for all screen time\n",
    "    \n",
    "    # Age affects usage patterns\n",
    "    user_age = np.random.randint(18, 65)\n",
    "    if user_age < 30:\n",
    "        # Younger users are more active\n",
    "        screen_time *= np.random.uniform(1.1, 1.4)\n",
    "        data_usage_mb *= np.random.uniform(1.2, 1.5)\n",
    "    elif user_age > 50:\n",
    "        # Older users are less active\n",
    "        screen_time *= np.random.uniform(0.6, 0.9)\n",
    "        data_usage_mb *= np.random.uniform(0.7, 0.9)\n",
    "    \n",
    "    device_data.append({\n",
    "        'User_ID': f'USER_{i+1:04d}',\n",
    "        'Device_Model': device,\n",
    "        'Operating_System': os,\n",
    "        'App_Usage_Time_hours': round(app_usage_time, 2),\n",
    "        'Screen_On_Time_hours': round(screen_time, 2),\n",
    "        'Battery_Drain_percent': round(battery_drain, 1),\n",
    "        'Number_of_Apps_Installed': int(num_apps),\n",
    "        'Data_Usage_MB': round(data_usage_mb, 1),\n",
    "        'Age': user_age,\n",
    "        'Device_Tier': device_tier  # For reference, normally not available\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "device_df = pd.DataFrame(device_data)\n",
    "\n",
    "print(f\"📊 Generated mobile device usage data for {len(device_df)} users\")\n",
    "print(f\"📱 Operating Systems: {device_df['Operating_System'].value_counts().to_dict()}\")\n",
    "print(f\"📱 Device Models: {device_df['Device_Model'].nunique()} unique models\")\n",
    "print(f\"📊 Average screen time: {device_df['Screen_On_Time_hours'].mean():.2f} hours/day\")\n",
    "print(f\"📊 Average data usage: {device_df['Data_Usage_MB'].mean():.0f} MB/month\")\n",
    "\n",
    "device_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One-hot encode Operating System and Device Model columns\n",
    "\n",
    "# Create a copy for encoding\n",
    "device_encoded = device_df.copy()\n",
    "\n",
    "print(\"🔄 One-Hot Encoding Device Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Operating System One-Hot Encoding\n",
    "print(\"📱 Operating System encoding...\")\n",
    "os_encoded = pd.get_dummies(device_encoded['Operating_System'], prefix='OS')\n",
    "print(f\"   Created {len(os_encoded.columns)} OS features: {list(os_encoded.columns)}\")\n",
    "\n",
    "# Device Model One-Hot Encoding  \n",
    "print(\"📱 Device Model encoding...\")\n",
    "device_model_encoded = pd.get_dummies(device_encoded['Device_Model'], prefix='Device')\n",
    "print(f\"   Created {len(device_model_encoded.columns)} device features\")\n",
    "print(f\"   Device features: {list(device_model_encoded.columns)}\")\n",
    "\n",
    "# Combine original features with encoded features\n",
    "device_encoded = pd.concat([\n",
    "    device_encoded.drop(['Operating_System', 'Device_Model'], axis=1),\n",
    "    os_encoded,\n",
    "    device_model_encoded\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\n📊 Enhanced dataset shape: {device_encoded.shape}\")\n",
    "print(f\"📊 Original features: {device_df.shape[1]}\")\n",
    "print(f\"📊 New features added: {device_encoded.shape[1] - device_df.shape[1]}\")\n",
    "\n",
    "# Show the distribution of encoded features\n",
    "print(f\"\\n📈 Operating System Distribution:\")\n",
    "for col in os_encoded.columns:\n",
    "    count = os_encoded[col].sum()\n",
    "    pct = count / len(device_encoded) * 100\n",
    "    print(f\"   {col}: {count} users ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📈 Top Device Models:\")\n",
    "device_counts = device_df['Device_Model'].value_counts().head(8)\n",
    "for device, count in device_counts.items():\n",
    "    pct = count / len(device_df) * 100\n",
    "    print(f\"   {device}: {count} users ({pct:.1f}%)\")\n",
    "\n",
    "device_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create interaction features for credit risk assessment\n",
    "\n",
    "print(\"⚙️ Creating Device Interaction Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create interaction features\n",
    "device_features = device_encoded.copy()\n",
    "\n",
    "# Key interaction features as specified\n",
    "print(\"📊 Core Interaction Features:\")\n",
    "\n",
    "# 1. Data usage per app\n",
    "device_features['data_usage_per_app'] = (\n",
    "    device_features['Data_Usage_MB'] / device_features['Number_of_Apps_Installed']\n",
    ")\n",
    "print(f\"   ✅ data_usage_per_app: Data Usage / Number of Apps\")\n",
    "print(f\"      Range: {device_features['data_usage_per_app'].min():.1f} - {device_features['data_usage_per_app'].max():.1f} MB/app\")\n",
    "\n",
    "# 2. Battery intensiveness  \n",
    "device_features['battery_intensiveness'] = (\n",
    "    device_features['Battery_Drain_percent'] / device_features['App_Usage_Time_hours']\n",
    ")\n",
    "print(f\"   ✅ battery_intensiveness: Battery Drain / App Usage Time\")\n",
    "print(f\"      Range: {device_features['battery_intensiveness'].min():.2f} - {device_features['battery_intensiveness'].max():.2f} %/hour\")\n",
    "\n",
    "# Additional valuable interaction features for credit assessment\n",
    "print(f\"\\n📊 Additional Credit-Relevant Features:\")\n",
    "\n",
    "# 3. Screen efficiency (screen time vs app usage time)\n",
    "device_features['screen_efficiency'] = (\n",
    "    device_features['App_Usage_Time_hours'] / device_features['Screen_On_Time_hours']\n",
    ")\n",
    "print(f\"   ✅ screen_efficiency: App Usage / Screen Time ratio\")\n",
    "\n",
    "# 4. App density (how many apps per hour of usage)\n",
    "device_features['app_density'] = (\n",
    "    device_features['Number_of_Apps_Installed'] / device_features['App_Usage_Time_hours']\n",
    ")\n",
    "print(f\"   ✅ app_density: Apps per hour of usage\")\n",
    "\n",
    "# 5. Data intensity (data usage per hour)\n",
    "device_features['data_intensity'] = (\n",
    "    device_features['Data_Usage_MB'] / device_features['Screen_On_Time_hours']\n",
    ")\n",
    "print(f\"   ✅ data_intensity: Data usage per screen hour\")\n",
    "\n",
    "# 6. Usage consistency (ratio of app usage to screen time)\n",
    "device_features['usage_consistency'] = (\n",
    "    device_features['App_Usage_Time_hours'] / device_features['Screen_On_Time_hours']\n",
    ")\n",
    "print(f\"   ✅ usage_consistency: How focused is device usage\")\n",
    "\n",
    "# 7. Power efficiency score\n",
    "device_features['power_efficiency'] = (\n",
    "    device_features['Screen_On_Time_hours'] / device_features['Battery_Drain_percent'] * 100\n",
    ")\n",
    "print(f\"   ✅ power_efficiency: Screen hours per battery %\")\n",
    "\n",
    "# Handle any infinite or NaN values\n",
    "device_features = device_features.replace([np.inf, -np.inf], np.nan)\n",
    "numeric_cols = ['data_usage_per_app', 'battery_intensiveness', 'screen_efficiency', \n",
    "                'app_density', 'data_intensity', 'usage_consistency', 'power_efficiency']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    median_val = device_features[col].median()\n",
    "    device_features[col] = device_features[col].fillna(median_val)\n",
    "\n",
    "print(f\"\\n📈 Feature Statistics:\")\n",
    "for col in numeric_cols:\n",
    "    values = device_features[col]\n",
    "    print(f\"   {col:.<25} {values.mean():.3f} ± {values.std():.3f}\")\n",
    "\n",
    "print(f\"\\n📊 Enhanced dataset now has {device_features.shape[1]} features\")\n",
    "device_features[['User_ID'] + numeric_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize device features and analyze patterns for credit risk\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "fig.suptitle('📱 Device Usage Pattern Analysis for Credit Risk', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Operating System vs Data Usage Per App\n",
    "ax1 = axes[0, 0]\n",
    "os_data_usage = []\n",
    "os_labels = []\n",
    "for os in device_features.columns:\n",
    "    if os.startswith('OS_'):\n",
    "        os_name = os.replace('OS_', '')\n",
    "        os_users = device_features[device_features[os] == 1]\n",
    "        if len(os_users) > 0:\n",
    "            os_data_usage.append(os_users['data_usage_per_app'].values)\n",
    "            os_labels.append(os_name)\n",
    "\n",
    "ax1.boxplot(os_data_usage, labels=os_labels)\n",
    "ax1.set_title('📊 Data Usage per App by Operating System')\n",
    "ax1.set_ylabel('Data Usage per App (MB)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Battery Intensiveness Distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(device_features['battery_intensiveness'], bins=30, alpha=0.7, color='orange')\n",
    "ax2.set_title('🔋 Battery Intensiveness Distribution')\n",
    "ax2.set_xlabel('Battery Drain per Usage Hour (%/hour)')\n",
    "ax2.set_ylabel('Number of Users')\n",
    "ax2.axvline(device_features['battery_intensiveness'].median(), color='red', linestyle='--', \n",
    "            label=f'Median: {device_features[\"battery_intensiveness\"].median():.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Device Tier vs Usage Patterns\n",
    "ax3 = axes[1, 0]\n",
    "premium_devices = ['iPhone 14', 'iPhone 13', 'Samsung Galaxy S23', 'Google Pixel 7', 'OnePlus 11']\n",
    "device_features['is_premium'] = device_features['Device_Tier'] == 'premium'\n",
    "\n",
    "premium_usage = device_features[device_features['is_premium']]['Screen_On_Time_hours']\n",
    "standard_usage = device_features[~device_features['is_premium']]['Screen_On_Time_hours']\n",
    "\n",
    "ax3.boxplot([premium_usage, standard_usage], labels=['Premium Devices', 'Standard Devices'])\n",
    "ax3.set_title('📱 Screen Time by Device Tier')\n",
    "ax3.set_ylabel('Screen Time (hours/day)')\n",
    "\n",
    "# Plot 4: Age vs Data Intensity\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(device_features['Age'], device_features['data_intensity'], \n",
    "                     c=device_features['is_premium'], cmap='viridis', alpha=0.6)\n",
    "ax4.set_title('📊 Age vs Data Intensity (Colored by Device Tier)')\n",
    "ax4.set_xlabel('User Age')\n",
    "ax4.set_ylabel('Data Intensity (MB/hour)')\n",
    "plt.colorbar(scatter, ax=ax4, label='Premium Device (1=Yes, 0=No)')\n",
    "\n",
    "# Plot 5: Feature Correlation Heatmap\n",
    "ax5 = axes[2, 0]\n",
    "correlation_features = ['data_usage_per_app', 'battery_intensiveness', 'screen_efficiency',\n",
    "                       'app_density', 'data_intensity', 'usage_consistency', 'power_efficiency']\n",
    "corr_matrix = device_features[correlation_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax5, fmt='.2f')\n",
    "ax5.set_title('🔄 Device Feature Correlations')\n",
    "\n",
    "# Plot 6: Credit Risk Indicators\n",
    "ax6 = axes[2, 1]\n",
    "# Create a simple risk score based on device patterns\n",
    "device_features['risk_score'] = (\n",
    "    (device_features['data_usage_per_app'] > device_features['data_usage_per_app'].quantile(0.8)).astype(int) +\n",
    "    (device_features['battery_intensiveness'] > device_features['battery_intensiveness'].quantile(0.8)).astype(int) +\n",
    "    (device_features['Screen_On_Time_hours'] > device_features['Screen_On_Time_hours'].quantile(0.8)).astype(int) +\n",
    "    (device_features['is_premium']).astype(int) * -1  # Premium devices reduce risk\n",
    ")\n",
    "\n",
    "risk_counts = device_features['risk_score'].value_counts().sort_index()\n",
    "ax6.bar(risk_counts.index, risk_counts.values, color=['green', 'yellow', 'orange', 'red'][:len(risk_counts)])\n",
    "ax6.set_title('📈 Device-Based Risk Score Distribution')\n",
    "ax6.set_xlabel('Risk Score')\n",
    "ax6.set_ylabel('Number of Users')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics by device characteristics\n",
    "print(\"\\n📊 DEVICE FEATURES ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n💰 Premium vs Standard Device Analysis:\")\n",
    "premium_stats = device_features[device_features['is_premium']].agg({\n",
    "    'data_usage_per_app': 'mean',\n",
    "    'battery_intensiveness': 'mean', \n",
    "    'Screen_On_Time_hours': 'mean',\n",
    "    'Data_Usage_MB': 'mean'\n",
    "})\n",
    "standard_stats = device_features[~device_features['is_premium']].agg({\n",
    "    'data_usage_per_app': 'mean',\n",
    "    'battery_intensiveness': 'mean',\n",
    "    'Screen_On_Time_hours': 'mean', \n",
    "    'Data_Usage_MB': 'mean'\n",
    "})\n",
    "\n",
    "for feature in premium_stats.index:\n",
    "    print(f\"   {feature:.<25} Premium: {premium_stats[feature]:.2f} | Standard: {standard_stats[feature]:.2f}\")\n",
    "\n",
    "print(f\"\\n📱 Operating System Patterns:\")\n",
    "for os_col in [col for col in device_features.columns if col.startswith('OS_')]:\n",
    "    os_name = os_col.replace('OS_', '')\n",
    "    os_users = device_features[device_features[os_col] == 1]\n",
    "    if len(os_users) > 0:\n",
    "        avg_data_per_app = os_users['data_usage_per_app'].mean()\n",
    "        avg_battery_int = os_users['battery_intensiveness'].mean()\n",
    "        print(f\"   {os_name:.<15} Data/App: {avg_data_per_app:.1f} MB | Battery Int: {avg_battery_int:.2f} %/hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aebf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Credit Risk Assessment using Device Features\n",
    "\n",
    "def assess_credit_risk_from_device(row):\n",
    "    \"\"\"\n",
    "    Assess credit risk based on device usage patterns.\n",
    "    Returns risk level and contributing factors.\n",
    "    \"\"\"\n",
    "    risk_factors = []\n",
    "    risk_score = 0\n",
    "    \n",
    "    # Device quality indicator\n",
    "    if row['is_premium']:\n",
    "        risk_score -= 1  # Premium devices suggest better financial status\n",
    "    else:\n",
    "        risk_factors.append(\"Standard device tier\")\n",
    "    \n",
    "    # Data usage patterns\n",
    "    if row['data_usage_per_app'] > row['data_usage_per_app'] * 1.5:  # High data usage per app\n",
    "        risk_factors.append(\"High data usage per app\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Battery usage patterns (very high might indicate financial stress - old device)\n",
    "    if row['battery_intensiveness'] > device_features['battery_intensiveness'].quantile(0.8):\n",
    "        risk_factors.append(\"High battery intensiveness\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Excessive screen time might indicate unemployment or underemployment\n",
    "    if row['Screen_On_Time_hours'] > device_features['Screen_On_Time_hours'].quantile(0.9):\n",
    "        risk_factors.append(\"Excessive screen time\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # Very low screen time might indicate device issues or financial constraints\n",
    "    if row['Screen_On_Time_hours'] < device_features['Screen_On_Time_hours'].quantile(0.1):\n",
    "        risk_factors.append(\"Very low device usage\")\n",
    "        risk_score += 1\n",
    "    \n",
    "    # App efficiency (many apps but low usage might indicate tech-savvy user)\n",
    "    if row['app_density'] > device_features['app_density'].quantile(0.8):\n",
    "        risk_score -= 0.5  # Slightly positive indicator\n",
    "    \n",
    "    # Data intensity (high data usage suggests engagement and potentially higher income)\n",
    "    if row['data_intensity'] > device_features['data_intensity'].quantile(0.7):\n",
    "        risk_score -= 0.5  # Slightly positive indicator\n",
    "    \n",
    "    # Age-based adjustments\n",
    "    if row['Age'] < 25:\n",
    "        # Young users with premium devices and high usage are typically lower risk\n",
    "        if row['is_premium'] and row['Screen_On_Time_hours'] > 4:\n",
    "            risk_score -= 0.5\n",
    "    elif row['Age'] > 55:\n",
    "        # Older users with consistent moderate usage are typically stable\n",
    "        if 2 <= row['Screen_On_Time_hours'] <= 5:\n",
    "            risk_score -= 0.5\n",
    "    \n",
    "    # Determine risk level\n",
    "    if risk_score <= 0:\n",
    "        risk_level = \"LOW\"\n",
    "    elif risk_score <= 2:\n",
    "        risk_level = \"MEDIUM\"\n",
    "    else:\n",
    "        risk_level = \"HIGH\"\n",
    "    \n",
    "    return risk_level, risk_score, risk_factors\n",
    "\n",
    "# Apply risk assessment to all users\n",
    "print(\"🎯 DEVICE-BASED CREDIT RISK ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "risk_results = []\n",
    "for idx, row in device_features.iterrows():\n",
    "    risk_level, risk_score, risk_factors = assess_credit_risk_from_device(row)\n",
    "    risk_results.append({\n",
    "        'User_ID': row['User_ID'],\n",
    "        'Risk_Level': risk_level,\n",
    "        'Risk_Score': risk_score,\n",
    "        'Risk_Factors': risk_factors\n",
    "    })\n",
    "\n",
    "risk_df = pd.DataFrame(risk_results)\n",
    "\n",
    "# Summary statistics\n",
    "risk_distribution = risk_df['Risk_Level'].value_counts()\n",
    "print(f\"📊 Risk Distribution:\")\n",
    "for level, count in risk_distribution.items():\n",
    "    pct = count / len(risk_df) * 100\n",
    "    print(f\"   {level:.<10} {count:>4} users ({pct:>5.1f}%)\")\n",
    "\n",
    "# Show examples by risk level\n",
    "print(f\"\\n👥 Sample Risk Assessments:\")\n",
    "for risk_level in ['LOW', 'MEDIUM', 'HIGH']:\n",
    "    sample = risk_df[risk_df['Risk_Level'] == risk_level].head(3)\n",
    "    print(f\"\\n🎯 {risk_level} RISK Examples:\")\n",
    "    \n",
    "    for idx, row in sample.iterrows():\n",
    "        user_data = device_features[device_features['User_ID'] == row['User_ID']].iloc[0]\n",
    "        print(f\"   👤 {row['User_ID']}:\")\n",
    "        print(f\"      📱 Device: {user_data['Device_Tier']} ({device_df[device_df['User_ID'] == row['User_ID']]['Device_Model'].iloc[0]})\")\n",
    "        print(f\"      📊 Screen Time: {user_data['Screen_On_Time_hours']:.1f} hrs\")\n",
    "        print(f\"      📶 Data/App: {user_data['data_usage_per_app']:.1f} MB\")\n",
    "        print(f\"      🔋 Battery Int.: {user_data['battery_intensiveness']:.2f} %/hr\")\n",
    "        print(f\"      👤 Age: {user_data['Age']}\")\n",
    "        print(f\"      ⚠️  Risk Factors: {', '.join(row['Risk_Factors']) if row['Risk_Factors'] else 'None'}\")\n",
    "\n",
    "# Device feature importance for credit decisions\n",
    "print(f\"\\n📈 DEVICE FEATURES FOR CREDIT ASSESSMENT:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "feature_importance = {\n",
    "    'data_usage_per_app': 'Data efficiency indicates financial awareness',\n",
    "    'battery_intensiveness': 'High values may suggest old/damaged device',\n",
    "    'screen_efficiency': 'Focused usage suggests purpose-driven behavior',\n",
    "    'app_density': 'Tech-savvy users often have better financial literacy',\n",
    "    'data_intensity': 'High data usage correlates with engagement and income',\n",
    "    'usage_consistency': 'Regular patterns indicate lifestyle stability',\n",
    "    'power_efficiency': 'Device maintenance suggests financial responsibility'\n",
    "}\n",
    "\n",
    "for feature, description in feature_importance.items():\n",
    "    avg_val = device_features[feature].mean()\n",
    "    print(f\"   📊 {feature:.<25} {description}\")\n",
    "    print(f\"      {'':.<25} Average: {avg_val:.3f}\")\n",
    "\n",
    "print(f\"\\n💡 Credit Risk Insights from Device Data:\")\n",
    "print(f\"   ✅ Premium device users show {(risk_df[device_features['is_premium']]['Risk_Level'] == 'LOW').mean()*100:.1f}% low risk rate\")\n",
    "print(f\"   📱 iOS users have {device_features[device_features['OS_iOS'] == 1]['data_usage_per_app'].mean():.1f} MB/app average\")\n",
    "print(f\"   🤖 Android users have {device_features[device_features['OS_Android'] == 1]['data_usage_per_app'].mean():.1f} MB/app average\")\n",
    "print(f\"   ⚡ High battery intensiveness (>{device_features['battery_intensiveness'].quantile(0.8):.2f}) affects {(device_features['battery_intensiveness'] > device_features['battery_intensiveness'].quantile(0.8)).sum()} users\")\n",
    "\n",
    "# Create final device feature summary for credit model\n",
    "device_summary = device_features.groupby('User_ID').agg({\n",
    "    'data_usage_per_app': 'first',\n",
    "    'battery_intensiveness': 'first',\n",
    "    'screen_efficiency': 'first',\n",
    "    'app_density': 'first',\n",
    "    'data_intensity': 'first',\n",
    "    'usage_consistency': 'first',\n",
    "    'power_efficiency': 'first',\n",
    "    'is_premium': 'first'\n",
    "}).round(3)\n",
    "\n",
    "# Add OS and device encoding\n",
    "os_cols = [col for col in device_features.columns if col.startswith('OS_')]\n",
    "device_cols = [col for col in device_features.columns if col.startswith('Device_')]\n",
    "\n",
    "for col in os_cols + device_cols:\n",
    "    device_summary[col] = device_features.groupby('User_ID')[col].first()\n",
    "\n",
    "print(f\"\\n📋 Device Feature Summary for Credit Model:\")\n",
    "print(f\"   📊 Features per user: {device_summary.shape[1]}\")\n",
    "print(f\"   👥 Users analyzed: {device_summary.shape[0]}\")\n",
    "\n",
    "device_summary.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
